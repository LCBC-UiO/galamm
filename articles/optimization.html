<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Optimization • galamm</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Optimization">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">galamm</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/galamm.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/lmm_factor.html">Linear Mixed Models with Factor Structures</a></li>
    <li><a class="dropdown-item" href="../articles/glmm_factor.html">Generalized Linear Mixed Models with Factor Structures</a></li>
    <li><a class="dropdown-item" href="../articles/lmm_heteroscedastic.html">Heteroscedastic Linear Mixed Models</a></li>
    <li><a class="dropdown-item" href="../articles/latent_observed_interaction.html">Interactions Between Latent and Observed Covariates</a></li>
    <li><a class="dropdown-item" href="../articles/mixed_response.html">Models with Mixed Response Types</a></li>
    <li><a class="dropdown-item" href="../articles/semiparametric.html">Semiparametric Latent Variable Modeling</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Advanced topics</h6></li>
    <li><a class="dropdown-item" href="../articles/scaling.html">Computational Scaling</a></li>
    <li><a class="dropdown-item" href="../articles/optimization.html">Optimization</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/LCBC-UiO/galamm/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Optimization</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/LCBC-UiO/galamm/blob/main/vignettes/optimization.Rmd" class="external-link"><code>vignettes/optimization.Rmd</code></a></small>
      <div class="d-none name"><code>optimization.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/LCBC-UiO/galamm" class="external-link">galamm</a></span><span class="op">)</span></span></code></pre></div>
<p>The purpose of this vignette is to describe the optimization
procedure used by <code>galamm</code>, and what kind of tools one can
use in the case of convergence issues.</p>
<div class="section level3">
<h3 id="high-level-overview">High-Level Overview<a class="anchor" aria-label="anchor" href="#high-level-overview"></a>
</h3>
<p>The optimization procedure used by <code>galamm</code> is described
in Section 3 of <span class="citation">Sørensen, Fjell, and Walhovd (<a href="#ref-sorensenLongitudinalModelingAgeDependent2023">2023</a>)</span>.
It consists of two steps:</p>
<ul>
<li><p>In the inner loop, the marginal likelihood is evaluated at a
given set of parameters. The marginal likelihood is what you obtain by
integrating out the random effects, and this integration is done with
the Laplace approximation. The Laplace approximation yields a large
system of equations that needs to be solved iteratively, except in the
case with conditionally Gaussian responses and unit link function, for
which a single step is sufficient to solve the system. When written in
matrix-vector form, this system of equations will in most cases have an
overwhelming majority of zeros, and to avoid wasting memory and time on
storing and multiplying zero, we use sparse matrix methods.</p></li>
<li><p>In the outer loop, we try to find the parameters that maximize
the marginal likelihood. For each new set of parameters, the whole
procedure in the inner loop has to be repeated. By default, we use the
limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm with box
constraints <span class="citation">(<a href="#ref-byrdLimitedMemoryAlgorithm1995">Byrd et al. 1995</a>)</span>,
abbreviated L-BFGS-B. In particular, we use the implementation in R’s
<code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code> function, which is obtained by setting
<code>method = "L-BFGS-B"</code>. L-BFGS-B requires first derivatives,
and these are obtained by automatic differentiation <span class="citation">(<a href="#ref-skaugAutomaticDifferentiationFacilitate2002">Skaug
2002</a>)</span>. In most use cases of <code>galamm</code>, we also use
constraints on some of the parameters, e.g., to ensure that variances
are non-negative. As an alternative, the Nelder-Mead algorithm with box
constraints <span class="citation">(<a href="#ref-batesFittingLinearMixedEffects2015">Bates et al. 2015</a>; <a href="#ref-nelderSimplexMethodFunction1965">Nelder and Mead
1965</a>)</span> from <code>lme4</code> is also available. Since the
Nelder-Mead algorithm is derivative free, automatic differentiation is
not used in this case, except for computing the Hessian matrix at the
final step.</p></li>
</ul>
<p>At convergence, the Hessian matrix of second derivatives is computed
exactly, again using automatic differentiation. The inverse of this
matrix is the covariance matrix of the parameter estimates, and is used
to compute Wald type confidence intervals.</p>
</div>
<div class="section level3">
<h3 id="modifying-the-l-bfgs-b-algorithm">Modifying the L-BFGS-B algorithm<a class="anchor" aria-label="anchor" href="#modifying-the-l-bfgs-b-algorithm"></a>
</h3>
<p>We will illustrate some ways of modifying the optimization procedure
with the covariate measurement model example shown in the <a href="https://lcbc-uio.github.io/galamm/articles/mixed_response.html">vignette
on models with mixed response types</a>. Here we start by simply setting
up what we need to fit the model.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">loading_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="cn">NA</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">families</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">gaussian</span>, <span class="va">binomial</span><span class="op">)</span></span>
<span><span class="va">family_mapping</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html" class="external-link">ifelse</a></span><span class="op">(</span><span class="va">diet</span><span class="op">$</span><span class="va">item</span> <span class="op">==</span> <span class="st">"chd"</span>, <span class="fl">2</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">formula</span> <span class="op">&lt;-</span> <span class="va">y</span> <span class="op">~</span> <span class="fl">0</span> <span class="op">+</span> <span class="va">chd</span> <span class="op">+</span> <span class="op">(</span><span class="va">age</span> <span class="op">*</span> <span class="va">bus</span><span class="op">)</span><span class="op">:</span><span class="va">chd</span> <span class="op">+</span> <span class="va">fiber</span> <span class="op">+</span></span>
<span>  <span class="op">(</span><span class="va">age</span> <span class="op">*</span> <span class="va">bus</span><span class="op">)</span><span class="op">:</span><span class="va">fiber</span> <span class="op">+</span> <span class="va">fiber2</span> <span class="op">+</span> <span class="op">(</span><span class="fl">0</span> <span class="op">+</span> <span class="va">loading</span> <span class="op">|</span> <span class="va">id</span><span class="op">)</span></span></code></pre></div>
<p>Fitting the model with default arguments yields a warning when we
look at the summary object.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/galamm.html">galamm</a></span><span class="op">(</span></span>
<span>  formula <span class="op">=</span> <span class="va">formula</span>,</span>
<span>  data <span class="op">=</span> <span class="va">diet</span>,</span>
<span>  family <span class="op">=</span> <span class="va">families</span>,</span>
<span>  family_mapping <span class="op">=</span> <span class="va">family_mapping</span>,</span>
<span>  factor <span class="op">=</span> <span class="st">"loading"</span>,</span>
<span>  load.var <span class="op">=</span> <span class="st">"item"</span>,</span>
<span>  lambda <span class="op">=</span> <span class="va">loading_matrix</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; Warning in vcov.galamm(object, parm = "lambda"): Rank deficient Hessian matrix.Could not compute covariance matrix.</span></span>
<span><span class="co">#&gt; Warning in vcov.galamm(object, "beta"): Rank deficient Hessian matrix.Could not compute covariance matrix.</span></span>
<span><span class="co">#&gt; GALAMM fit by maximum marginal likelihood.</span></span>
<span><span class="co">#&gt; Formula: formula</span></span>
<span><span class="co">#&gt;    Data: diet</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      AIC      BIC   logLik deviance df.resid </span></span>
<span><span class="co">#&gt;   2837.6   2892.9  -1406.8  12529.3      730 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Lambda:</span></span>
<span><span class="co">#&gt;         loading SE</span></span>
<span><span class="co">#&gt; lambda1   1.000  .</span></span>
<span><span class="co">#&gt; lambda2   1.000  .</span></span>
<span><span class="co">#&gt; lambda3  -2.026  .</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups Name    Variance Std.Dev.</span></span>
<span><span class="co">#&gt;  id     loading 0        0       </span></span>
<span><span class="co">#&gt; Number of obs: 742, groups:  id, 333</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error z value Pr(&gt;|z|)</span></span>
<span><span class="co">#&gt; chd           -1.78692         NA      NA       NA</span></span>
<span><span class="co">#&gt; fiber         17.96184         NA      NA       NA</span></span>
<span><span class="co">#&gt; fiber2        -0.64927         NA      NA       NA</span></span>
<span><span class="co">#&gt; chd:age        0.06682         NA      NA       NA</span></span>
<span><span class="co">#&gt; chd:bus       -0.06882         NA      NA       NA</span></span>
<span><span class="co">#&gt; fiber:age     -0.20480         NA      NA       NA</span></span>
<span><span class="co">#&gt; fiber:bus     -1.69601         NA      NA       NA</span></span>
<span><span class="co">#&gt; chd:age:bus   -0.04934         NA      NA       NA</span></span>
<span><span class="co">#&gt; fiber:age:bus  0.16097         NA      NA       NA</span></span></code></pre></div>
<p>In this case, we can increase the amount of information provided by
<code>optim</code>, with the <code>trace</code> argument. To avoid
getting too much output, we also reduce the number of iterations. We set
the <code>control</code> argument as follows:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">control</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/galamm_control.html">galamm_control</a></span><span class="op">(</span>optim_control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>maxit <span class="op">=</span> <span class="fl">5</span>, trace <span class="op">=</span> <span class="fl">3</span>, REPORT <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Here, <code>maxit = 5</code> means that we take at most 5 iterations,
<code>trace = 3</code> means that we want more information from
L-BFGS-B, and <code>REPORT= = 1</code> means that we want L-BFGS-B to
report information at each step it takes. We provide this object to the
<code>control</code> argument in <code>galamm</code>, and rerun the
model:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/galamm.html">galamm</a></span><span class="op">(</span></span>
<span>  formula <span class="op">=</span> <span class="va">formula</span>,</span>
<span>  data <span class="op">=</span> <span class="va">diet</span>,</span>
<span>  family <span class="op">=</span> <span class="va">families</span>,</span>
<span>  family_mapping <span class="op">=</span> <span class="va">family_mapping</span>,</span>
<span>  factor <span class="op">=</span> <span class="st">"loading"</span>,</span>
<span>  load.var <span class="op">=</span> <span class="st">"item"</span>,</span>
<span>  lambda <span class="op">=</span> <span class="va">loading_matrix</span>,</span>
<span>  control <span class="op">=</span> <span class="va">control</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; N = 11, M = 20 machine precision = 2.22045e-16</span></span>
<span><span class="co">#&gt; At X0, 0 variables are exactly at the bounds</span></span>
<span><span class="co">#&gt; At iterate     0  f=         2148  |proj g|=       122.68</span></span>
<span><span class="co">#&gt; At iterate     1  f =       2132.1  |proj g|=        275.51</span></span>
<span><span class="co">#&gt; At iterate     2  f =       2100.1  |proj g|=         193.7</span></span>
<span><span class="co">#&gt; At iterate     3  f =       1975.6  |proj g|=        177.11</span></span>
<span><span class="co">#&gt; At iterate     4  f =       1923.2  |proj g|=         165.7</span></span>
<span><span class="co">#&gt; At iterate     5  f =       1898.8  |proj g|=        83.839</span></span>
<span><span class="co">#&gt; At iterate     6  f =       1887.9  |proj g|=        49.147</span></span>
<span><span class="co">#&gt; final  value 1887.871646 </span></span>
<span><span class="co">#&gt; stopped after 6 iterations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; Warning in vcov.galamm(mod): Rank deficient Hessian matrix.Could not compute covariance matrix.</span></span>
<span><span class="co">#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]</span></span>
<span><span class="co">#&gt;  [1,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [2,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [3,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [4,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [5,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [6,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [7,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [8,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [9,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span></code></pre></div>
<p>Since what we did was simply to turn in more reporting, it is no
surprise that the Hessian is still rank deficient, but from the output,
it is also clear that there are no obvious errors, like values that
diverge to infinity. The latter may also happen from time to time.</p>
<p>By default, L-BFGS-B uses the last 5 evaluations of the gradient to
approximate the Hessian that is used during optimization (not to be
confused with the exact Hessian compute with automatic differentiation
after convergence). We try to increase this to 25, and see if that makes
a difference. This is done with the <code>lmm</code> argument. We also
reduce the amount of reporting to be every 10th step, and avoid setting
the maximum number of iterations, which means that
<code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code>’s default option is used.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">control</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/galamm_control.html">galamm_control</a></span><span class="op">(</span>optim_control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>trace <span class="op">=</span> <span class="fl">3</span>, REPORT <span class="op">=</span> <span class="fl">10</span>, lmm <span class="op">=</span> <span class="fl">25</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>It is clear that neither this solved the issue.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/galamm.html">galamm</a></span><span class="op">(</span></span>
<span>  formula <span class="op">=</span> <span class="va">formula</span>,</span>
<span>  data <span class="op">=</span> <span class="va">diet</span>,</span>
<span>  family <span class="op">=</span> <span class="va">families</span>,</span>
<span>  family_mapping <span class="op">=</span> <span class="va">family_mapping</span>,</span>
<span>  factor <span class="op">=</span> <span class="st">"loading"</span>,</span>
<span>  load.var <span class="op">=</span> <span class="st">"item"</span>,</span>
<span>  lambda <span class="op">=</span> <span class="va">loading_matrix</span>,</span>
<span>  control <span class="op">=</span> <span class="va">control</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; N = 11, M = 25 machine precision = 2.22045e-16</span></span>
<span><span class="co">#&gt; At X0, 0 variables are exactly at the bounds</span></span>
<span><span class="co">#&gt; At iterate     0  f=         2148  |proj g|=       122.68</span></span>
<span><span class="co">#&gt; At iterate    10  f =       1770.3  |proj g|=        30.656</span></span>
<span><span class="co">#&gt; At iterate    20  f =       1467.2  |proj g|=        11.286</span></span>
<span><span class="co">#&gt; At iterate    30  f =         1413  |proj g|=        4.3102</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; iterations 38</span></span>
<span><span class="co">#&gt; function evaluations 44</span></span>
<span><span class="co">#&gt; segments explored during Cauchy searches 39</span></span>
<span><span class="co">#&gt; BFGS updates skipped 0</span></span>
<span><span class="co">#&gt; active bounds at final generalized Cauchy point 1</span></span>
<span><span class="co">#&gt; norm of the final projected gradient 0.001689</span></span>
<span><span class="co">#&gt; final function value 1406.8</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; F = 1406.8</span></span>
<span><span class="co">#&gt; final  value 1406.801104 </span></span>
<span><span class="co">#&gt; converged</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; Warning in vcov.galamm(mod): Rank deficient Hessian matrix.Could not compute covariance matrix.</span></span>
<span><span class="co">#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]</span></span>
<span><span class="co">#&gt;  [1,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [2,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [3,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [4,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [5,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [6,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [7,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [8,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span>
<span><span class="co">#&gt;  [9,]   NA   NA   NA   NA   NA   NA   NA   NA   NA</span></span></code></pre></div>
<p>Looking at the model output again, we see that the random effect
variance is estimated to be exactly zero.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; Warning in vcov.galamm(object, parm = "lambda"): Rank deficient Hessian matrix.Could not compute covariance matrix.</span></span>
<span><span class="co">#&gt; Warning in vcov.galamm(object, "beta"): Rank deficient Hessian matrix.Could not compute covariance matrix.</span></span>
<span><span class="co">#&gt; GALAMM fit by maximum marginal likelihood.</span></span>
<span><span class="co">#&gt; Formula: formula</span></span>
<span><span class="co">#&gt;    Data: diet</span></span>
<span><span class="co">#&gt; Control: control</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      AIC      BIC   logLik deviance df.resid </span></span>
<span><span class="co">#&gt;   2837.6   2892.9  -1406.8  12529.3      730 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Lambda:</span></span>
<span><span class="co">#&gt;         loading SE</span></span>
<span><span class="co">#&gt; lambda1   1.000  .</span></span>
<span><span class="co">#&gt; lambda2   1.000  .</span></span>
<span><span class="co">#&gt; lambda3  -1.922  .</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups Name    Variance Std.Dev.</span></span>
<span><span class="co">#&gt;  id     loading 0        0       </span></span>
<span><span class="co">#&gt; Number of obs: 742, groups:  id, 333</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error z value Pr(&gt;|z|)</span></span>
<span><span class="co">#&gt; chd           -1.78673         NA      NA       NA</span></span>
<span><span class="co">#&gt; fiber         17.96179         NA      NA       NA</span></span>
<span><span class="co">#&gt; fiber2        -0.64904         NA      NA       NA</span></span>
<span><span class="co">#&gt; chd:age        0.06685         NA      NA       NA</span></span>
<span><span class="co">#&gt; chd:bus       -0.06894         NA      NA       NA</span></span>
<span><span class="co">#&gt; fiber:age     -0.20479         NA      NA       NA</span></span>
<span><span class="co">#&gt; fiber:bus     -1.69628         NA      NA       NA</span></span>
<span><span class="co">#&gt; chd:age:bus   -0.04937         NA      NA       NA</span></span>
<span><span class="co">#&gt; fiber:age:bus  0.16092         NA      NA       NA</span></span></code></pre></div>
<p>These types of obviously wrong zero variance estimates are well-known
for users of mixed models <span class="citation">(<a href="#ref-hodgesRichlyParameterizedLinear2013">Hodges 2013</a>)</span>.
We see if increasing the initial value for the variance parameter solves
the issue. This is done with the <code>start</code> argument to
<code>galamm</code>. The start argument requires a named list, with
optional arguments <code>theta</code>, <code>beta</code>,
<code>lambda</code>, and <code>weights</code>, giving initial values for
each of these groups of parameters. In this case <code>theta</code> is
the standard deviation of the random effect, and we increase it to 10 to
see what happens. By default, the initial value equals 1.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/galamm.html">galamm</a></span><span class="op">(</span></span>
<span>  formula <span class="op">=</span> <span class="va">formula</span>,</span>
<span>  data <span class="op">=</span> <span class="va">diet</span>,</span>
<span>  family <span class="op">=</span> <span class="va">families</span>,</span>
<span>  family_mapping <span class="op">=</span> <span class="va">family_mapping</span>,</span>
<span>  factor <span class="op">=</span> <span class="st">"loading"</span>,</span>
<span>  load.var <span class="op">=</span> <span class="st">"item"</span>,</span>
<span>  lambda <span class="op">=</span> <span class="va">loading_matrix</span>,</span>
<span>  start <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>theta <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>,</span>
<span>  control <span class="op">=</span> <span class="va">control</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; N = 11, M = 25 machine precision = 2.22045e-16</span></span>
<span><span class="co">#&gt; At X0, 0 variables are exactly at the bounds</span></span>
<span><span class="co">#&gt; At iterate     0  f=       2827.6  |proj g|=       123.31</span></span>
<span><span class="co">#&gt; At iterate    10  f =       1764.5  |proj g|=        103.86</span></span>
<span><span class="co">#&gt; At iterate    20  f =       1623.6  |proj g|=        131.81</span></span>
<span><span class="co">#&gt; At iterate    30  f =       1447.9  |proj g|=        75.096</span></span>
<span><span class="co">#&gt; At iterate    40  f =       1400.5  |proj g|=        35.591</span></span>
<span><span class="co">#&gt; At iterate    50  f =       1373.1  |proj g|=        3.3359</span></span>
<span><span class="co">#&gt; At iterate    60  f =       1372.2  |proj g|=     0.0016541</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; iterations 60</span></span>
<span><span class="co">#&gt; function evaluations 72</span></span>
<span><span class="co">#&gt; segments explored during Cauchy searches 61</span></span>
<span><span class="co">#&gt; BFGS updates skipped 0</span></span>
<span><span class="co">#&gt; active bounds at final generalized Cauchy point 0</span></span>
<span><span class="co">#&gt; norm of the final projected gradient 0.00165415</span></span>
<span><span class="co">#&gt; final function value 1372.16</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; F = 1372.16</span></span>
<span><span class="co">#&gt; final  value 1372.160386 </span></span>
<span><span class="co">#&gt; converged</span></span></code></pre></div>
<p>Now we see that the model converged and that the Hessian is no longer
rank deficient.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; GALAMM fit by maximum marginal likelihood.</span></span>
<span><span class="co">#&gt; Formula: formula</span></span>
<span><span class="co">#&gt;    Data: diet</span></span>
<span><span class="co">#&gt; Control: control</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      AIC      BIC   logLik deviance df.resid </span></span>
<span><span class="co">#&gt;   2768.3   2823.6  -1372.2   2002.9      730 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Lambda:</span></span>
<span><span class="co">#&gt;         loading      SE</span></span>
<span><span class="co">#&gt; lambda1  1.0000       .</span></span>
<span><span class="co">#&gt; lambda2  1.0000       .</span></span>
<span><span class="co">#&gt; lambda3 -0.1339 0.05121</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups Name    Variance Std.Dev.</span></span>
<span><span class="co">#&gt;  id     loading 23.64    4.862   </span></span>
<span><span class="co">#&gt; Number of obs: 742, groups:  id, 333</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error  z value   Pr(&gt;|z|)</span></span>
<span><span class="co">#&gt; chd           -1.91525    0.27229 -7.03373  2.011e-12</span></span>
<span><span class="co">#&gt; fiber         17.94849    0.48686 36.86601 1.620e-297</span></span>
<span><span class="co">#&gt; fiber2         0.22402    0.41783  0.53614  5.919e-01</span></span>
<span><span class="co">#&gt; chd:age        0.06615    0.05931  1.11531  2.647e-01</span></span>
<span><span class="co">#&gt; chd:bus       -0.02895    0.34355 -0.08427  9.328e-01</span></span>
<span><span class="co">#&gt; fiber:age     -0.21204    0.10090 -2.10135  3.561e-02</span></span>
<span><span class="co">#&gt; fiber:bus     -1.68303    0.63721 -2.64123  8.261e-03</span></span>
<span><span class="co">#&gt; chd:age:bus   -0.04999    0.06507 -0.76822  4.424e-01</span></span>
<span><span class="co">#&gt; fiber:age:bus  0.16818    0.11223  1.49854  1.340e-01</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="optimization-with-the-nelder-mead-algorithm">Optimization with the Nelder-Mead algorithm<a class="anchor" aria-label="anchor" href="#optimization-with-the-nelder-mead-algorithm"></a>
</h3>
<p>The Nelder-Mead algorithm is turned on by setting
<code>method = "Nelder-Mead"</code> when calling
<code><a href="../reference/galamm_control.html">galamm_control()</a></code>. We also turn on reporting every 20th
function evaluation by setting <code>verbose = 1</code>:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">control</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/galamm_control.html">galamm_control</a></span><span class="op">(</span></span>
<span>  optim_control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>verbose <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"Nelder-Mead"</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>We provide the estimates obtained with the L-BFGS-B algorithm as
initial values. For this we can use the convenience function
<code>extract_optim_parameters</code>:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">start</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/extract_optim_parameters.galamm.html">extract_optim_parameters</a></span><span class="op">(</span><span class="va">mod</span><span class="op">)</span></span></code></pre></div>
<p>We now fit the model, providing the initial values to the
<code>start</code> argument.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mod_nm</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/galamm.html">galamm</a></span><span class="op">(</span></span>
<span>  formula <span class="op">=</span> <span class="va">formula</span>,</span>
<span>  data <span class="op">=</span> <span class="va">diet</span>,</span>
<span>  family <span class="op">=</span> <span class="va">families</span>,</span>
<span>  family_mapping <span class="op">=</span> <span class="va">family_mapping</span>,</span>
<span>  factor <span class="op">=</span> <span class="st">"loading"</span>,</span>
<span>  load.var <span class="op">=</span> <span class="st">"item"</span>,</span>
<span>  lambda <span class="op">=</span> <span class="va">loading_matrix</span>,</span>
<span>  control <span class="op">=</span> <span class="va">control</span>,</span>
<span>  start <span class="op">=</span> <span class="va">start</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; (NM) 20: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 40: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 60: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 80: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 100: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 120: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 140: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 160: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 180: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 200: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 220: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 240: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 260: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 280: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 300: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 320: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 340: f = 1372.16 at    1.84246   -1.91525    17.9485   0.224017   0.066146 -0.0289499  -0.212035   -1.68303 -0.0499864   0.168178  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 360: f = 1372.16 at    1.84247   -1.91525    17.9485   0.223968  0.0661412  -0.028921   -0.21203   -1.68308 -0.0499804   0.168172  -0.133908</span></span>
<span><span class="co">#&gt; (NM) 380: f = 1372.16 at    1.84247   -1.91525    17.9485    0.22398  0.0661421 -0.0289289  -0.212034   -1.68304 -0.0499816   0.168174  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 400: f = 1372.16 at    1.84247   -1.91525    17.9485   0.223986  0.0661415 -0.0289274  -0.212031   -1.68305 -0.0499811   0.168171  -0.133909</span></span>
<span><span class="co">#&gt; (NM) 420: f = 1372.16 at    1.84247   -1.91525    17.9485   0.223985  0.0661434 -0.0289358  -0.212032   -1.68304 -0.0499824   0.168172  -0.133908</span></span></code></pre></div>
<p>The summary output shows that Nelder-Mead found exactly the same
optimum in this particular case, which is not surprising given the
intial values that we provided.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">mod_nm</span><span class="op">)</span></span>
<span><span class="co">#&gt; GALAMM fit by maximum marginal likelihood.</span></span>
<span><span class="co">#&gt; Formula: formula</span></span>
<span><span class="co">#&gt;    Data: diet</span></span>
<span><span class="co">#&gt; Control: control</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      AIC      BIC   logLik deviance df.resid </span></span>
<span><span class="co">#&gt;   2768.3   2823.6  -1372.2   2002.9      730 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Lambda:</span></span>
<span><span class="co">#&gt;         loading      SE</span></span>
<span><span class="co">#&gt; lambda1  1.0000       .</span></span>
<span><span class="co">#&gt; lambda2  1.0000       .</span></span>
<span><span class="co">#&gt; lambda3 -0.1339 0.05121</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups Name    Variance Std.Dev.</span></span>
<span><span class="co">#&gt;  id     loading 23.64    4.862   </span></span>
<span><span class="co">#&gt; Number of obs: 742, groups:  id, 333</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error z value   Pr(&gt;|z|)</span></span>
<span><span class="co">#&gt; chd           -1.91525    0.27229 -7.0337  2.011e-12</span></span>
<span><span class="co">#&gt; fiber         17.94851    0.48686 36.8660 1.618e-297</span></span>
<span><span class="co">#&gt; fiber2         0.22398    0.41783  0.5360  5.919e-01</span></span>
<span><span class="co">#&gt; chd:age        0.06614    0.05931  1.1153  2.647e-01</span></span>
<span><span class="co">#&gt; chd:bus       -0.02893    0.34355 -0.0842  9.329e-01</span></span>
<span><span class="co">#&gt; fiber:age     -0.21203    0.10090 -2.1013  3.561e-02</span></span>
<span><span class="co">#&gt; fiber:bus     -1.68305    0.63721 -2.6413  8.260e-03</span></span>
<span><span class="co">#&gt; chd:age:bus   -0.04998    0.06507 -0.7682  4.424e-01</span></span>
<span><span class="co">#&gt; fiber:age:bus  0.16817    0.11223  1.4985  1.340e-01</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="implementation-details">Implementation Details<a class="anchor" aria-label="anchor" href="#implementation-details"></a>
</h3>
<p>At a given set of parameters, the marginal likelihood is evaluated
completely in C++. For solving the penalized iteratively reweighted
least squares problem arising due to the Laplace approximation, we use
sparse matrix methods from the Eigen C++ template library through the
<code>RcppEigen</code> package <span class="citation">(<a href="#ref-batesFastElegantNumerical2013">Bates and Eddelbuettel
2013</a>)</span>. In order to keep track of the derivatives throughout
this iterative process, we use the <a href="https://autodiff.github.io/" class="external-link">autodiff library</a> <span class="citation">(<a href="#ref-lealAutodiffModernFast2018">Leal
2018</a>)</span>. However, since <code>autodiff</code> natively only
supports dense matrix operations with <code>Eigen</code>, we have
extended this library so that it also supports sparse matrix operations.
This modified version of the <code>autodiff</code> library can be found
at <code>inst/include/autodiff/</code>.</p>
<p>In order to maximize the marginal likelihood, we currently rely on
the <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code> function in R. To make use of the fact that
both the marginal likelihood value itself and first derivatives are
returned from the C++ function, we use memoisation, provided by the
<code>memoise</code> package <span class="citation">(<a href="#ref-wickhamMemoiseMemoisationFunctions2021">Wickham et al.
2021</a>)</span>. However, the optimization process still involves
copying all model data between R and C++ for each new set of parameters.
This is potentially an efficiency bottleneck with large datasets,
although with the limited profiling that has been done so far, it seems
like the vast majority of the computation time is spent actually solving
the penalized iteratively reweighted least squares problem in C++.</p>
</div>
<div class="section level3">
<h3 id="future-improvements">Future Improvements<a class="anchor" aria-label="anchor" href="#future-improvements"></a>
</h3>
<p>We aim to perform also the outer optimization loop in C++, to avoid
copying data back and forth between R and C++ during optimization. This
requires finding an off-the-shelf optimization routine which is as good
as the L-BFGS-B implementation provided by <code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">optim()</a></code>, and
which plays well with <code>autodiff</code>.</p>
<p>In addition, the current implementation uses only forward mode
automatic differentiation. In the future, we aim to add backward mode as
an option, as this might turn out to be more efficient for problems with
a large number of variables.</p>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-batesFastElegantNumerical2013" class="csl-entry">
Bates, Douglas M, and Dirk Eddelbuettel. 2013. <span>“Fast and
<span>Elegant Numerical Linear Algebra Using</span> the <span>RcppEigen
Package</span>.”</span> <em>Journal of Statistical Software</em> 52
(February): 1–24. <a href="https://doi.org/10.18637/jss.v052.i05" class="external-link">https://doi.org/10.18637/jss.v052.i05</a>.
</div>
<div id="ref-batesFittingLinearMixedEffects2015" class="csl-entry">
Bates, Douglas M, Martin Mächler, Ben Bolker, and Steve Walker. 2015.
<span>“Fitting <span>Linear Mixed-Effects Models Using</span>
Lme4.”</span> <em>Journal of Statistical Software</em> 67 (1): 1–48. <a href="https://doi.org/10.18637/jss.v067.i01" class="external-link">https://doi.org/10.18637/jss.v067.i01</a>.
</div>
<div id="ref-byrdLimitedMemoryAlgorithm1995" class="csl-entry">
Byrd, Richard H., Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. 1995.
<span>“A <span>Limited Memory Algorithm</span> for <span>Bound
Constrained Optimization</span>.”</span> <em>SIAM Journal on Scientific
Computing</em> 16 (5): 1190–1208. <a href="https://doi.org/10.1137/0916069" class="external-link">https://doi.org/10.1137/0916069</a>.
</div>
<div id="ref-hodgesRichlyParameterizedLinear2013" class="csl-entry">
Hodges, James S. 2013. <em>Richly <span>Parameterized Linear Models
Additive</span>, <span>Time Series</span>, and <span>Spatial Models
Using Random Effects</span></em>. 1st ed. Chapman &amp;
<span>Hall</span>/<span>CRC Texts</span> in <span>Statistical
Science</span>. Chapman &amp; Hall.
</div>
<div id="ref-lealAutodiffModernFast2018" class="csl-entry">
Leal, Allan M. M. 2018. <span>“Autodiff, a Modern, Fast and Expressive
<span>C</span>++ Library for Automatic Differentiation.”</span>
</div>
<div id="ref-nelderSimplexMethodFunction1965" class="csl-entry">
Nelder, J. A., and R. Mead. 1965. <span>“A <span>Simplex Method</span>
for <span>Function Minimization</span>.”</span> <em>The Computer
Journal</em> 7 (4): 308–13. <a href="https://doi.org/10.1093/comjnl/7.4.308" class="external-link">https://doi.org/10.1093/comjnl/7.4.308</a>.
</div>
<div id="ref-skaugAutomaticDifferentiationFacilitate2002" class="csl-entry">
Skaug, Hans J. 2002. <span>“Automatic <span>Differentiation</span> to
<span>Facilitate Maximum Likelihood Estimation</span> in <span>Nonlinear
Random Effects Models</span>.”</span> <em>Journal of Computational and
Graphical Statistics</em> 11 (2): 458–70.
</div>
<div id="ref-sorensenLongitudinalModelingAgeDependent2023" class="csl-entry">
Sørensen, Øystein, Anders M. Fjell, and Kristine B. Walhovd. 2023.
<span>“Longitudinal <span>Modeling</span> of <span>Age-Dependent Latent
Traits</span> with <span>Generalized Additive Latent</span> and
<span>Mixed Models</span>.”</span> <em>Psychometrika</em> 88 (2):
456–86. <a href="https://doi.org/10.1007/s11336-023-09910-z" class="external-link">https://doi.org/10.1007/s11336-023-09910-z</a>.
</div>
<div id="ref-wickhamMemoiseMemoisationFunctions2021" class="csl-entry">
Wickham, Hadley, Jim Hester, Winston Chang, Kirill Müller, and Daniel
Cook. 2021. <em>Memoise: ’Memoisation’ of Functions</em>. Manual.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Øystein Sørensen.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.2.</p>
</div>

    </footer>
</div>





  </body>
</html>
