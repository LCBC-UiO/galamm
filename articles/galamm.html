<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Introduction • galamm</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Introduction">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">galamm</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.2.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/galamm.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/lmm_factor.html">Linear Mixed Models with Factor Structures</a></li>
    <li><a class="dropdown-item" href="../articles/glmm_factor.html">Generalized Linear Mixed Models with Factor Structures</a></li>
    <li><a class="dropdown-item" href="../articles/lmm_heteroscedastic.html">Heteroscedastic Linear Mixed Models</a></li>
    <li><a class="dropdown-item" href="../articles/latent_observed_interaction.html">Interactions Between Latent and Observed Covariates</a></li>
    <li><a class="dropdown-item" href="../articles/mixed_response.html">Models with Mixed Response Types</a></li>
    <li><a class="dropdown-item" href="../articles/semiparametric.html">Semiparametric Latent Variable Modeling</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Advanced topics</h6></li>
    <li><a class="dropdown-item" href="../articles/scaling.html">Computational Scaling</a></li>
    <li><a class="dropdown-item" href="../articles/optimization.html">Optimization</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/LCBC-UiO/galamm/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Introduction</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/LCBC-UiO/galamm/blob/main/vignettes/galamm.Rmd" class="external-link"><code>vignettes/galamm.Rmd</code></a></small>
      <div class="d-none name"><code>galamm.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/LCBC-UiO/galamm" class="external-link">galamm</a></span><span class="op">)</span></span></code></pre></div>
<p>This vignette is aimed to give you a high-level overview of the types
of models supported by the galamm package, and to point you to relevant
vignettes where you can find more information.</p>
<div class="section level3">
<h3 id="generalized-additive-latent-and-mixed-models">Generalized Additive Latent and Mixed Models<a class="anchor" aria-label="anchor" href="#generalized-additive-latent-and-mixed-models"></a>
</h3>
<p>Generalized additive latent and mixed models (GALAMMs) <span class="citation">(<a href="#ref-sorensenLongitudinalModelingAgeDependent2023">Sørensen,
Fjell, and Walhovd 2023</a>)</span> is an extension of generalized
linear latent and mixed models (GLLAMMs) <span class="citation">(<a href="#ref-rabe-heskethGeneralizedMultilevelStructural2004">Rabe-Hesketh,
Skrondal, and Pickles 2004</a>; <a href="#ref-skrondalGeneralizedLatentVariable2004">Skrondal and
Rabe-Hesketh 2004</a>)</span> which allows both observed responses and
latent variables to depend smoothly on observed variables.
<em>Smoothly</em> here means that the relationship is not assumed to
follow a particular parametric form, e.g., as specified by a linear
model. Instead, an <em>a priori</em> assumption is made that the
relationship is smooth, and the model then attempts to learn the
relationship from the data. GALAMM uses smoothing splines to obtain
this, identically to how generalized additive models (GAMs) <span class="citation">(<a href="#ref-woodGeneralizedAdditiveModels2017">Wood
2017</a>)</span> are estimated.</p>
<p>The GLLAMM framework contains many elements which are currently not
implemented in the galamm package. This includes both nonparametric
random effects and a large number of model families, e.g., for censored
responses. If you need any of this, but not semiparametric estimation,
the Stata based <a href="http://www.gllamm.org/" class="external-link">GLLAMM package</a> is
likely the place you should go. Conversely, galamm incorporates crossed
random effects easily and efficiently, while these are hard to specify
using GLLAMM.</p>
<div class="section level4">
<h4 id="response-model">Response Model<a class="anchor" aria-label="anchor" href="#response-model"></a>
</h4>
<p>GALAMMs are specified using three building blocks. First,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
responses
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">y_{1}, \dots, y_{n}</annotation></semantics></math>
are assumed independently distributed according to an exponential family
with density</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo>,</mo><mi>ϕ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>y</mi><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>b</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mi>ϕ</mi></mfrac><mo>+</mo><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>,</mo><mi>ϕ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
f\left(y | \theta, \phi\right) = \exp \left( \frac{y\theta(\mu) - b\left(\theta(\mu)\right)}{\phi} + c\left(y, \phi\right) \right)
</annotation></semantics></math></p>
<p>here
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>ν</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mu = g^{-1}(\nu)</annotation></semantics></math>
is the mean,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g^{-1}(\cdot)</annotation></semantics></math>
is the inverse of link function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ν</mi><annotation encoding="application/x-tex">\nu</annotation></semantics></math>
is a “nonlinear predictor”,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>
is a dispersion parameter, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">b(\cdot)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">c(\cdot)</annotation></semantics></math>
are known functions. In contrast to what is assumed, e.g., by <a href="https://cran.r-project.org/package=lme4" class="external-link">lme4</a> <span class="citation">(<a href="#ref-batesFittingLinearMixedEffects2015">Bates et al.
2015</a>)</span>, the functions
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">b(\cdot)</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">c(\cdot)</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation></semantics></math>
are allowed to vary between observations. That is, the observations can
come from different members of the exponential family. The vignette on
<a href="https://lcbc-uio.github.io/galamm/articles/mixed_response.html">models
with mixed response types</a> describes this in detail.</p>
<p>Using canonical link functions, the response model simplifies to</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>ν</mi><mo>,</mo><mi>ϕ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>y</mi><mi>ν</mi><mo>−</mo><mi>b</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ν</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mi>ϕ</mi></mfrac><mo>+</mo><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>,</mo><mi>ϕ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
f\left(y | \nu, \phi\right) = \exp \left( \frac{y\nu - b\left(\nu\right)}{\phi} + c\left(y, \phi\right) \right)
</annotation></semantics></math></p>
</div>
<div class="section level4">
<h4 id="nonlinear-predictor">Nonlinear Predictor<a class="anchor" aria-label="anchor" href="#nonlinear-predictor"></a>
</h4>
<p>Next, the nonlinear predictor, which corresponds to the measurement
model in a classical structural equation model, is defined by</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ν</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><msub><mi>f</mi><mi>s</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>2</mn></mrow><mi>L</mi></munderover><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mi>l</mi></msub></munderover><msubsup><mi>η</mi><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><msubsup><mi>𝐳</mi><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><msup><mrow></mrow><mi>′</mi></msup><msubsup><mi>𝛌</mi><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>,</mo></mrow><annotation encoding="application/x-tex">
\nu = \sum_{s=1}^{S} f_{s}\left(\mathbf{x}\right) + \sum_{l=2}^{L}\sum_{m=1}^{M_{l}} \eta_{m}^{(l)} \mathbf{z}^{(l)}_{m}{}^{'}\boldsymbol{\lambda}_{m}^{(l)},
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>
are explanatory variables,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>s</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{s}(\mathbf{x})</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">s=1,\dots,S</annotation></semantics></math>
are smooth functions,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>η</mi><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\eta_{m}^{(l)}</annotation></semantics></math>
are latent variables varying at level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>𝛌</mi><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><msup><mrow></mrow><mi>T</mi></msup><msubsup><mi>𝐳</mi><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\boldsymbol{\lambda}_{m}^{(l)}{}^{T} \mathbf{z}_{m}^{(l)}</annotation></semantics></math>
is the weighted sum of a vector of explanatory variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>𝐳</mi><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\mathbf{z}_{m}^{(l)}</annotation></semantics></math>
varying at level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
and parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>𝛌</mi><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\boldsymbol{\lambda}_{m}^{(l)}</annotation></semantics></math>.
Let</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝛈</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">[</mo><msubsup><mi>η</mi><mn>1</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>,</mo><mi>…</mi><mo>,</mo><msubsup><mi>η</mi><msub><mi>M</mi><mi>l</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">]</mo></mrow><mi>T</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>M</mi><mi>l</mi></msub></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{\eta}^{(l)} = [\eta_{1}^{(l)}, \dots, \eta_{M_{l}}^{(l)}]^{T} \in \mathbb{R}^{M_{l}}</annotation></semantics></math></p>
<p>be the vector of all latent variables at level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>,
and</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝛈</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>𝛈</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>,</mo><mi>…</mi><mo>,</mo><msup><mi>𝛈</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">]</mo></mrow><mi>T</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>M</mi></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{\eta} = [\boldsymbol{\eta}^{(2)}, \dots, \boldsymbol{\eta}^{(L)}]^{T} \in \mathbb{R}^{M}</annotation></semantics></math></p>
<p>the vector of all latent variables belonging to a given level-2 unit,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>2</mn></mrow><mi>L</mi></msubsup><msub><mi>M</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">M = \sum_{l=2}^{L} M_{l}</annotation></semantics></math>.
The word “level” is here used to denote a grouping level; they are not
necessarily hierarchical.</p>
</div>
</div>
<div class="section level3">
<h3 id="structural-model">Structural Model<a class="anchor" aria-label="anchor" href="#structural-model"></a>
</h3>
<p>The structural model specifies how the latent variables are related
to each other and to observed variables, and is given by</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝛈</mi><mo>=</mo><mi>𝐁</mi><mi>𝛈</mi><mo>+</mo><mi>𝐡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>𝛇</mi></mrow><annotation encoding="application/x-tex">
\boldsymbol{\eta} = \mathbf{B}\boldsymbol{\eta} + \mathbf{h}\left(\mathbf{w}\right)
+ \boldsymbol{\zeta}
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐁</mi><annotation encoding="application/x-tex">\mathbf{B}</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">M \times M</annotation></semantics></math>
matrix of regression coefficients for regression among latent variables
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐰</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>Q</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{w} \in \mathbb{R}^{Q}</annotation></semantics></math>
is a vector of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
predictors for the latent variables.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>𝐡</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>𝐡</mi><mi>L</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>M</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{h}(\mathbf{w}) = [\mathbf{h}_{2}(\mathbf{w}), \dots, \mathbf{h}_{L}(\mathbf{w})] \in \mathbb{R}^{M}</annotation></semantics></math>
is a vector of smooth functions whose components
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐡</mi><mi>l</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>M</mi><mi>l</mi></msub></msup></mrow><annotation encoding="application/x-tex">\mathbf{h}_{l}(\mathbf{w}) \in \mathbb{R}^{M_{l}}</annotation></semantics></math>
are vectors of functions predicting the latent variables varying at
level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>,
and depending on a subset of the elements
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐰</mi><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛇</mi><annotation encoding="application/x-tex">\boldsymbol{\zeta}</annotation></semantics></math>
is a vector of normally distributed random effects,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝛇</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>𝟎</mn><mo>,</mo><msup><mi>𝚿</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\boldsymbol{\zeta}^{(l)} \sim N(\mathbf{0}, \boldsymbol{\Psi}^{(l)})</annotation></semantics></math>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mo>=</mo><mn>2</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">l=2,\dots,L</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝚿</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>M</mi><mi>l</mi></msub><mo>×</mo><msub><mi>M</mi><mi>l</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{\Psi}^{(l)} \in \mathbb{R}^{M_{l} \times M_{l}}</annotation></semantics></math>
is the covariance matrix of random effects at level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>.
Defining the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">M \times M</annotation></semantics></math>
covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝚿</mi><mo>=</mo><mtext mathvariant="normal">diag</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝚿</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>,</mo><mi>…</mi><mo>,</mo><msup><mi>𝚿</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\boldsymbol{\Psi} = \text{diag}(\boldsymbol{\Psi}^{(2)}, \dots, \boldsymbol{\Psi}^{(L)})</annotation></semantics></math>,
we also have
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝛇</mi><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝚿</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\boldsymbol{\zeta} \sim N(\mathbf{0}, \boldsymbol{\Psi})</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="mixed-model-representation">Mixed Model Representation<a class="anchor" aria-label="anchor" href="#mixed-model-representation"></a>
</h3>
<p>In <span class="citation">Sørensen, Fjell, and Walhovd (<a href="#ref-sorensenLongitudinalModelingAgeDependent2023">2023</a>)</span>
we show that any model specified as above can be transformed to a
GLLAMM, which is essentially a generalized nonlinear mixed model. This
transformation is rather complex, so we won’t spell it out here, but the
key steps are:</p>
<ol style="list-style-type: decimal">
<li>Converting smooth terms to their mixed model form.</li>
<li>Estimate the resulting GLLAMM.</li>
<li>Convert back to the original parametrization.</li>
</ol>
<p>In galamm we use the same transformations as the <a href="https://CRAN.R-project.org/package=gamm4" class="external-link">gamm4</a> package
does.</p>
</div>
<div class="section level3">
<h3 id="maximum-marginal-likelihood-estimation">Maximum Marginal Likelihood Estimation<a class="anchor" aria-label="anchor" href="#maximum-marginal-likelihood-estimation"></a>
</h3>
<p>In mixed model representation, the nonlinear predictor can be written
on the form</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝛎</mi><mo>=</mo><mi>𝐗</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛌</mi><mo>,</mo><mi>𝐁</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>𝛃</mi><mo>+</mo><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛌</mi><mo>,</mo><mi>𝐁</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>𝛇</mi></mrow><annotation encoding="application/x-tex">
\boldsymbol{\nu} = \mathbf{X}(\boldsymbol{\lambda}, \mathbf{B}) \boldsymbol{\beta} +  \mathbf{Z}(\boldsymbol{\lambda}, \mathbf{B}) \boldsymbol{\zeta}
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐗</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛌</mi><mo>,</mo><mi>𝐁</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{X}(\boldsymbol{\lambda}, \mathbf{B})</annotation></semantics></math>
is the regression matrix for fixed effects
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛌</mi><mo>,</mo><mi>𝐁</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Z}(\boldsymbol{\lambda}, \mathbf{B})</annotation></semantics></math>
is the regression matrix for random effects
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛇</mi><annotation encoding="application/x-tex">\boldsymbol{\zeta}</annotation></semantics></math>.
In contrast to with generalized linear mixed models, however, both
matrices will in general depend on factor loadings
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛌</mi><annotation encoding="application/x-tex">\boldsymbol{\lambda}</annotation></semantics></math>
and regression coefficients between latent variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐁</mi><annotation encoding="application/x-tex">\mathbf{B}</annotation></semantics></math>.
Both of these are parameters that need to be estimated, and hence
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐗</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛌</mi><mo>,</mo><mi>𝐁</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{X}(\boldsymbol{\lambda}, \mathbf{B})</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛌</mi><mo>,</mo><mi>𝐁</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Z}(\boldsymbol{\lambda}, \mathbf{B})</annotation></semantics></math>
need to be updated throughout the estimation process.</p>
<div class="section level4">
<h4 id="evaluating-the-marginal-likelihood">Evaluating the Marginal Likelihood<a class="anchor" aria-label="anchor" href="#evaluating-the-marginal-likelihood"></a>
</h4>
<p>Plugging the nonlinear predictor into the structural model, we obtain
the joint likelihood for the model. We then obtain the marginal
likelihood by integrating over the random effects, yielding a marginal
likelihood function of the form</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛃</mi><mo>,</mo><mi>𝚲</mi><mo>,</mo><mi>𝚪</mi><mo>,</mo><mi>𝛌</mi><mo>,</mo><mi>𝐁</mi><mo>,</mo><mi>𝛟</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>π</mi><msub><mi>ϕ</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mi>r</mi><mi>/</mi><mn>2</mn></mrow></msup><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>r</mi></msup></msub><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛃</mi><mo>,</mo><mi>𝚲</mi><mo>,</mo><mi>𝚪</mi><mo>,</mo><mi>𝛌</mi><mo>,</mo><mi>𝐁</mi><mo>,</mo><mi>𝛟</mi><mo>,</mo><mi>𝐮</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mtext mathvariant="normal">d</mtext><mi>𝐮</mi></mrow><annotation encoding="application/x-tex">
L\left(\boldsymbol{\beta}, \boldsymbol{\Lambda}, \boldsymbol{\Gamma}, \boldsymbol{\lambda}, \mathbf{B}, \boldsymbol{\phi}\right) =  \left(2 \pi \phi_{1}\right)^{-r/2}  \int_{\mathbb{R}^{r}} \exp\left( g\left(\boldsymbol{\beta}, \boldsymbol{\Lambda}, \boldsymbol{\Gamma}, \boldsymbol{\lambda}, \mathbf{B}, \boldsymbol{\phi}, \mathbf{u}\right) \right) \text{d} \mathbf{u}
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>
is a standardized version of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛇</mi><annotation encoding="application/x-tex">\boldsymbol{\zeta}</annotation></semantics></math>.
In order to evaluate the marginal likelihood at a given set of parameter
values, we use the Laplace approximation combined with sparse matrix
operations, extending <span class="citation">Bates et al. (<a href="#ref-batesFittingLinearMixedEffects2015">2015</a>)</span>’s
algorithm for linear mixed models.</p>
</div>
<div class="section level4">
<h4 id="maximizing-the-marginal-likelihood">Maximizing the Marginal Likelihood<a class="anchor" aria-label="anchor" href="#maximizing-the-marginal-likelihood"></a>
</h4>
<p>We obtain maximum marginal likelihood estimates by maximizing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛃</mi><mo>,</mo><mi>𝚲</mi><mo>,</mo><mi>𝚪</mi><mo>,</mo><mi>𝛌</mi><mo>,</mo><mi>𝐁</mi><mo>,</mo><mi>𝛟</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L\left(\boldsymbol{\beta}, \boldsymbol{\Lambda}, \boldsymbol{\Gamma}, \boldsymbol{\lambda}, \mathbf{B}, \boldsymbol{\phi}\right)</annotation></semantics></math>,
subject to possible constraints, e.g., that variances are non-negative.
For this, we use the L-BFGS-B algorithm implement in
<code><a href="https://rdrr.io/r/stats/optim.html" class="external-link">stats::optim</a></code>. The predicted values of random effects,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>𝐮</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\widehat{\mathbf{u}}</annotation></semantics></math>
are obtained as posterior modes at the final estimates.</p>
</div>
</div>
<div class="section level3">
<h3 id="example-models">Example Models<a class="anchor" aria-label="anchor" href="#example-models"></a>
</h3>
<p>To see how galamm is used in practice, take a look at the vignettes
describing models with different components.</p>
<ul>
<li>
<a href="https://lcbc-uio.github.io/galamm/articles/lmm_factor.html">Linear
mixed models with factor structures</a>.</li>
<li>
<a href="https://lcbc-uio.github.io/galamm/articles/glmm_factor.html">Generalized
linear mixed models with factor structures</a>.</li>
<li>
<a href="https://lcbc-uio.github.io/galamm/articles/lmm_heteroscedastic.html">Linear
mixed models with heteroscedastic residuals</a>.</li>
<li>
<a href="https://lcbc-uio.github.io/galamm/articles/latent_observed_interaction.html">Models
with interactions between latent and observed covariates</a>.</li>
<li>
<a href="https://lcbc-uio.github.io/galamm/articles/mixed_response.html">Mixed
models with mixed response types</a>.</li>
<li>
<a href="https://lcbc-uio.github.io/galamm/articles/semiparametric.html">Generalized
additive mixed models with factor structures</a>.</li>
</ul>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-batesFittingLinearMixedEffects2015" class="csl-entry">
Bates, Douglas M, Martin Mächler, Ben Bolker, and Steve Walker. 2015.
<span>“Fitting <span>Linear Mixed-Effects Models Using</span>
Lme4.”</span> <em>Journal of Statistical Software</em> 67 (1): 1–48. <a href="https://doi.org/10.18637/jss.v067.i01" class="external-link">https://doi.org/10.18637/jss.v067.i01</a>.
</div>
<div id="ref-rabe-heskethGeneralizedMultilevelStructural2004" class="csl-entry">
Rabe-Hesketh, Sophia, Anders Skrondal, and Andrew Pickles. 2004.
<span>“Generalized Multilevel Structural Equation Modeling.”</span>
<em>Psychometrika</em> 69 (2): 167–90. <a href="https://doi.org/10.1007/BF02295939" class="external-link">https://doi.org/10.1007/BF02295939</a>.
</div>
<div id="ref-skrondalGeneralizedLatentVariable2004" class="csl-entry">
Skrondal, Anders, and Sophia Rabe-Hesketh. 2004. <em>Generalized Latent
Variable Modeling</em>. Interdisciplinary <span>Statistics
Series</span>. Boca Raton, Florida: <span>Chapman and Hall/CRC</span>.
</div>
<div id="ref-sorensenLongitudinalModelingAgeDependent2023" class="csl-entry">
Sørensen, Øystein, Anders M. Fjell, and Kristine B. Walhovd. 2023.
<span>“Longitudinal <span>Modeling</span> of <span>Age-Dependent Latent
Traits</span> with <span>Generalized Additive Latent</span> and
<span>Mixed Models</span>.”</span> <em>Psychometrika</em> 88 (2):
456–86. <a href="https://doi.org/10.1007/s11336-023-09910-z" class="external-link">https://doi.org/10.1007/s11336-023-09910-z</a>.
</div>
<div id="ref-woodGeneralizedAdditiveModels2017" class="csl-entry">
Wood, Simon N. 2017. <em>Generalized Additive Models: <span>An</span>
Introduction with <span>R</span></em>. 2nd ed. <span>Chapman and
Hall/CRC</span>.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Øystein Sørensen.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
