---
title: "Maximum Likelihood Estimation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MaximumLikelihood}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ../inst/references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(galamm)
```


The `galamm` package includes a function for computing the Laplace approximate marginal likelihood of both GLLAMMs and GALAMMs. In the future, this will replace the profile likelihood method, but due to the complexity of the matter, it will take some time to develop a decent API. This vignette describes how to perform maximum likelihood estimation with the currently available functions; it is complicated, but also gives the benefit of understanding what's going on, in addition to a more accurate algorithm which scales better. Before continuing, make sure you've read the Introduction vignette.

## Linear mixed model with factor structure

We will use an example dataset from the `PLmixed` package [@rockwoodEstimatingComplexMeasurement2019]. `PLmixed` is probably a better choice for this model, but this way we get a gentle start before we try estimating generalized multilevel models with semiparametric terms.

```{r}
library(PLmixed)
data("IRTsim")
IRTsub <- IRTsim[IRTsim$item < 4, ]
set.seed(12345)
IRTsub <- IRTsub[sample(nrow(IRTsub), 300), ]
IRTsub <- IRTsub[order(IRTsub$item), ]
irt.lam = c(1, NA, NA)
```

### Estimation with PLmixed

The model can be fit with `PLmixed` as follows, where we're pretending for now the the response is normally distributed, although the binomial distribution would be correct.

```{r, cache=TRUE}
form <- y ~ 0 + as.factor(item) + (0 + abil.sid |sid) +(0 + abil.sid |school)
irt.model <- PLmixed(form,
                     data = IRTsub, load.var = c("item"), # family = binomial,
                     REML = FALSE, factor = list(c("abil.sid")), 
                     lambda = list(irt.lam), iter.count = FALSE)
```

### Estimation with the galamm package

We can use the `marginal_likelihood` function from `galamm` to compute the marginal likelihood of this model at given values of fixed effects $\beta$, variance components $\theta$, and factor loadings $\lambda$.

We start by adding $\lambda$ as a variable in the model:

```{r}
dat <- IRTsub
dat$abil.sid <- 1
```

We then generate the components of a linear mixed model:

```{r}
library(lme4)
lmod <- lFormula(form, data = dat, REML = FALSE)
```

The fixed effects matrix does not contain any factor loadings, so we keep it as is.

```{r}
X <- lmod$X
```

The random effects matrix does contain factor loadings, so we need a mapping. `Zt` is on compressed sparse column format, and `diff(Zt@p)` thus shows us the number of entries in each column of this transposed matrix.

```{r}
Zt <- lmod$reTrms$Zt
table(diff(Zt@p))
lambda_mapping_Zt <- rep(dat$item, each = 2) - 2L
```

We also extract the random effects covariance matrix, with its mapping to $\theta$. See @batesFittingLinearMixedEffects2015 for details.

```{r}
Lambdat <- lmod$reTrms$Lambdat
theta_mapping <- lmod$reTrms$Lind - 1L
```

We can now start by confirming that we are able to compute the likelihood of the model.

```{r}
ml <- marginal_likelihood(
  y = dat$y,
  trials = rep(1, length(dat$y)),
  X = X,
  Zt = Zt,
  Lambdat = Lambdat,
  beta = fixef(irt.model),
  theta = getME(irt.model$lme4, "theta"),
  theta_mapping = theta_mapping,
  lambda = irt.model$Lambda[[1]][, "abil.sid", drop = TRUE][2:3],
  lambda_mapping_X = integer(),
  lambda_mapping_Zt = lambda_mapping_Zt,
  family = "gaussian",
  maxit_conditional_modes = 1
)
```

The `marginal_likelihood` function returns the log-likelihood, which we can compare with that from `PLmixed`:

```{r}
irt.model$`Log-Likelihood`
ml$logLik
```

The `marginal_likelihood` function also returns the gradient of the marginal likelihood with respect to $\theta$, $\beta$, and $\lambda$, in that order. Reassuringly, it is close to zero. Not however that `PLmixed` returns the maximum of the profile likelihood, which albeit consistent, is not the same as the maximum likelihood estimate [@gongPseudoMaximumLikelihood1981;@parkePseudoMaximumLikelihood1986;@pawitanAllLikelihood2001].

```{r}
ml$gradient
```

This gradient is computed with algorithmic differentiation using the `autodiff` C++ library [@lealAutodiffModernFast2018], which means that it is exact to computer precision.

We can use this gradient to find the maximum likelihood estimates of the model (technically, the Laplace approximate marginal maximum likelihood estimates). We also use `memoise` to avoid evaluating the function unnecessarily.

```{r}
library(memoise)
theta_inds <- 1:2
beta_inds <- 3:5
lambda_inds <- 6:7
bounds <- c(0, 0, rep(-Inf, 5))

mlwrapper <- function(par){
  marginal_likelihood(
    y = dat$y,
    trials = rep(1, length(dat$y)),
    X = X,
    Zt = Zt,
    Lambdat = Lambdat,
    beta = par[beta_inds],
    theta = par[theta_inds],
    theta_mapping = theta_mapping,
    lambda = par[lambda_inds],
    lambda_mapping_X = integer(),
    lambda_mapping_Zt = lambda_mapping_Zt,
    family = "gaussian",
    maxit_conditional_modes = 1
    )
}

mlmem <- memoise(mlwrapper)
fn <- function(par){
  mlmem(par)$logLik
}
gr <- function(par){
  mlmem(par)$gradient
}

```

We start the optimization from some pretty arbitrary values, and it converges quite fast, to a solution which is identical to the one the `PLmixed` found. We set `fnscale = -1` to get the *negative* log-likelihood, which `optim` then minimizes.

```{r}
par_init <- c(1, 1, 0, 0, 0, 1, 1)
opt <- optim(par_init, fn = fn, gr = gr, 
             method = "L-BFGS-B", lower = bounds,
             control = list(fnscale = -1), hessian = TRUE)

opt$value
irt.model$`Log-Likelihood`
```

We can confirm that the parameters are almost identical:

```{r, fig.width=4}
plot(
  opt$par, 
  c(getME(irt.model$lme4, "theta"), fixef(irt.model), 
    irt.model$Lambda[[1]][2:3, 1]),
  xlab = "optim()", ylab = "PLmixed");
abline(0, 1)
```

It should also be clear to the reader the `PLmixed` is much more convenient to use. However, the optimization shown here is faster:

```{r}
system.time({
  opt <- optim(par_init, fn = fn, gr = gr, 
             method = "L-BFGS-B", lower = bounds,
             control = list(fnscale = -1), hessian = TRUE)
})
```

```{r}
system.time({
  irt.model <- PLmixed(form,
                     data = IRTsub, load.var = c("item"),
                     REML = FALSE, factor = list(c("abil.sid")), 
                     lambda = list(irt.lam), iter.count = FALSE)
})
```

The inverse of the negative Hessian matrix is the covariance matrix of the model.

```{r}
S <- solve(-opt$hessian)
```

We can confirm that also these standard errors agree with `PLmixed`. Both for $\hat{\beta}$

```{r}
sqrt(diag(S))[beta_inds]
summary(irt.model)$`Fixed Effects`
```

and for $\hat{\lambda}$.

```{r}
sqrt(diag(S))[lambda_inds]
summary(irt.model)$Lambda
```


## Generalized linear mixed models

For illustrating use with generalized linear mixed models, we start by using a model without any factor structures.

We first fit the model using `lme4`:

```{r}
glmod <- glFormula(cbind(incidence, size - incidence) ~ period + (1 | herd),
                   data = cbpp, family = binomial)
devfun <- do.call(mkGlmerDevfun, glmod)
opt1 <- optimizeGlmer(devfun)
devfun <- updateGlmerDevfun(devfun, glmod$reTrms)
opt2 <- optimizeGlmer(devfun, stage=2)
fMod <- mkMerMod(environment(devfun), opt2, glmod$reTrms, fr = glmod$fr)
```

Then we do it using `marginal_likelihood`:

```{r}
theta_inds <- 1
beta_inds <- 2:5

mlwrapper <- function(par){
  marginal_likelihood(
    y = cbpp$incidence,
    trials = cbpp$size,
    X = glmod$X,
    Zt = glmod$reTrms$Zt,
    Lambdat = glmod$reTrms$Lambdat,
    beta = par[beta_inds],
    theta = par[theta_inds],
    theta_mapping = glmod$reTrms$Lind - 1L,
    lambda = numeric(),
    lambda_mapping_X = integer(),
    lambda_mapping_Zt = integer(),
    family = "binomial",
    maxit_conditional_modes = 50
  )
}

mlmem <- memoise(mlwrapper)
fn <- function(par){
  mlmem(par)$logLik
}
gr <- function(par){
  mlmem(par)$gradient
}

opt <- optim(
  par = c(1, rep(0, 4)), fn = fn, gr = gr,
  method = "L-BFGS-B", lower = c(0, rep(-Inf, 4)),
  control = list(fnscale = -1, maxit = 2000), 
  hessian = TRUE
)
```

Using `optim()` with L-BFGS-B yields slightly likelihood deviance than `lme4`.

```{r}
opt$value
logLik(fMod)
```

On the other hand, for given parameter values, they almost perfectly agree, confirming that our implementation is correct, but that some work is needed to check the optimization algorithms.

```{r}
fn(opt2$par)
```

Also the estimated standard errors are close.

```{r}
sqrt(diag(solve(-opt$hessian)))[beta_inds]
sqrt(diag(vcov(fMod)))
```


## Generalized additive mixed model with factor structures

```{r}
library(mgcv)
library(gamm4)
```


We can now fit the model illustrates in the Introduction vignette, which we refer to for details. We start by adding a weight column to `dat1`, to hold factor loadings.

```{r}
lambda_init <- c(item1 = 1, item2 = 2, item3 = .4)
dat1$weight <- lambda_init[dat1$item]
head(dat1)
```

To confirm that what we do is correct, we fit this GAMM at the initial values of the loadings.

```{r}
mod0 <- gamm4(y ~ 0 + s(x, by = weight), random = ~(0 + weight|id), 
              data = dat1, REML = FALSE)
```


We then compute the mixed model representation of the GAMM.

```{r}
sm <- smoothCon(s(x, by = weight), data = dat1)[[1]]
re <- smooth2random(sm, "", type = 2)
```

Then set up the list which holds the data.

```{r}
mdat <- list(
  id = dat1$id,
  y = dat1$y,
  Xf = re$Xf,
  Xr = re$rand$Xr,
  weight = dat1$weight,
  pseudoGroups = rep(1:ncol(re$rand$Xr), length = nrow(dat1))
)
```

And set up the model.

```{r}
lmod <- lFormula(y ~ 0 + Xf + (1 | pseudoGroups) + (0 + weight | id), 
                 data = mdat, REML = FALSE)
```

Then we add the penalized part of the smooth terms.

```{r}
lmod$reTrms$Ztlist$`1 | pseudoGroups` <- as(t(as.matrix(mdat$Xr))[], class(lmod$reTrms$Zt))
lmod$reTrms$Zt <- rbind(lmod$reTrms$Ztlist$`0 + weight | id`, 
                        lmod$reTrms$Ztlist$`1 | pseudoGroups`)
```

To begin with, we complete model fitting at the initial loading.

```{r}
devfun <- do.call(mkLmerDevfun, lmod)
opt <- optimizeLmer(devfun)
mod1 <- mkMerMod(environment(devfun), opt, lmod$reTrms, fr = lmod$fr)
```

We can now compare the fitted models. The likelihoods are equal.

```{r}
logLik(mod0$mer)
logLik(mod1)
```

As are the fixed effects and variance components.

```{r}
fixef(mod0$mer)
fixef(mod1)

getME(mod0$mer, "theta")
getME(mod1, "theta")
```

We can also compare the penalized spline coefficients.

```{r, fig.width=4}
plot(
  ranef(mod0$mer)$Xr[[1]],
  ranef(mod1)$pseudoGroups[[1]],
  xlab = "gamm4", ylab = "lme4"); abline(0, 1)
```

Having gained some confidence in our ability to fit a GAMM as a mixed model, we can now move on to also estimating the factor loadings. For this we use the `marginal_likelihood` function provided by this package, and we start by confirming that we get the same likelihood as with the two approaches shown above. 

We first remove the factor loading from the design matrices. The lambda mapping for `X` runs through each column.

```{r}
lambda_mapping_X <- rep(as.integer(factor(dat1$item)) - 2L, ncol(lmod$X))
```

The lambda mapping for `Zt` runs through the vector of structural nonzeros. Compressed sparse column format is used, so the `x` vector runs through columns. Each loading is repeated nine times:

```{r}
table(diff(lmod$reTrms$Zt@p))
```


```{r}
lambda_mapping_Zt <- rep(-1:1, each = 9, times = 300)
```

```{r}
X <- lmod$X / as.numeric(lambda_init[lambda_mapping_X + 2L])
Zt <- lmod$reTrms$Zt
Zt@x <- Zt@x / as.numeric(lambda_init[lambda_mapping_Zt + 2L])
```

We can confirm that they now are free of factor loadings:

```{r}
# Before
head(lmod$X)
# After
head(X)
```

```{r}
compmat <- cbind(before = lmod$reTrms$Zt@x, after = Zt@x)
compmat <- cbind(compmat, ratio = compmat[, 1] / compmat[, 2])

head(compmat, 27)
```

We also remove the estimated values from `Lambdat`.

```{r}
Lambdat <- lmod$reTrms$Lambdat
Lambdat@x <- rep(1, length(Lambdat@x))
```

We then compute the marginal likelihood.

```{r}
margl <- marginal_likelihood(
  y = mdat$y,
  trials = numeric(),
  X = X,
  Zt = Zt,
  Lambdat = Lambdat,
  beta = fixef(mod1),
  theta = getME(mod1, "theta"),
  theta_mapping = lmod$reTrms$Lind - 1L,
  lambda = lambda_init[-1],
  lambda_mapping_X = lambda_mapping_X,
  lambda_mapping_Zt = lambda_mapping_Zt,
  family = "gaussian",
  maxit_conditional_modes = 1
)

margl$logLik
logLik(mod1)
```

Finally, we can fit the model directly, using the same memoization technique as above.

```{r}
theta_inds <- 1:2
beta_inds <- 3:4
lambda_inds <- 5:6
bounds <- c(0, 0, rep(-Inf, 4))

mlwrapper <- function(par){
  marginal_likelihood(
    y = mdat$y,
    trials = numeric(),
    X = X,
    Zt = Zt,
    Lambdat = Lambdat,
    beta = par[beta_inds],
    theta = par[theta_inds],
    theta_mapping = lmod$reTrms$Lind - 1L,
    lambda = par[lambda_inds],
    lambda_mapping_X = lambda_mapping_X,
    lambda_mapping_Zt = lambda_mapping_Zt,
    family = "gaussian",
    maxit_conditional_modes = 1
  )
}

mlmem <- memoise(mlwrapper)
fn <- function(par){
  mlmem(par)$logLik
}
gr <- function(par){
  mlmem(par)$gradient
}

```

It ends up at the right values. Profile likelihood estimation fails from these initial values, and is much slower. Note that the deviance is lower now, because the factor loadings are estimated, rather than fixed at $(1, 2, .4)$.

```{r}
par_init <- c(1, 1, 0, 0, 1, 1)

opt <- optim(par_init, fn = fn, gr = gr, 
           method = "L-BFGS-B", lower = bounds, 
           control = list(fnscale = -1, maxit = 2000),
           hessian = TRUE)

opt$value
logLik(mod1)
```


```{r}
opt
```





## References
