---
title: "Maximum Likelihood Estimation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MaximumLikelihood}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ../inst/references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(galamm)
```


The `galamm` package includes a function for computing the Laplace approximate marginal likelihood of both GLLAMMs and GALAMMs. In the future, this will replace the profile likelihood method, but due to the complexity of the matter, it will take some time to develop a decent API. This vignette describes how to perform maximum likelihood estimation with the currently available functions; it is complicated, but also gives the benefit of understanding what's going on, in addition to a more accurate algorithm which scales better. Before continuing, make sure you've read the Introduction vignette.

## Linear mixed model with factor structure

We will use an example dataset from the `PLmixed` package [@rockwoodEstimatingComplexMeasurement2019]. The `PLmixed` is probably a better choice for this model, but this way we get a gentle start before we try estimating generalized multilevel models with semiparametric terms.

```{r}
library(PLmixed)
data("IRTsim")
IRTsub <- IRTsim[IRTsim$item < 4, ]
set.seed(12345)
IRTsub <- IRTsub[sample(nrow(IRTsub), 300), ]
IRTsub <- IRTsub[order(IRTsub$item), ]
irt.lam = c(1, NA, NA)
```


### Estimation with PLmixed

The model can be fit with `PLmixed` as follows, where we're pretending for now the the response is normally distributed, although the binomial distribution would be correct.

```{r, cache=TRUE}
form <- y ~ 0 + as.factor(item) + (0 + abil.sid |sid) +(0 + abil.sid |school)
irt.model <- PLmixed(form,
                     data = IRTsub, load.var = c("item"), # family = binomial,
                     REML = FALSE, factor = list(c("abil.sid")), 
                     lambda = list(irt.lam), iter.count = FALSE)
```

### Estimation with the galamm package

We can use the `marginal_likelihood` function from `galamm` to compute the marginal likelihood of this model at given values of fixed effects $\beta$, variance components $\theta$, and factor loadings $\lambda$.

We start by adding $\lambda$ as a variable in the model:

```{r}
dat <- IRTsub
lambda_init <- c(1, 2, 3)
dat$abil.sid <- lambda_init[dat$item]
```

We then generate the components of a linear mixed model:

```{r}
library(lme4)
lmod <- lFormula(form, data = dat, REML = FALSE)
```

The fixed effects matrix does not contain any factor loadings, so we keep it as is.

```{r}
X <- lmod$X
```

The random effects matrix does contain factor loadings, so we need a mapping. Also note that it is sparse. In the mapping, $-1$ denotes an element of $\lambda$ which is fixed. In addition, since C++ has base zero indexing, we need to subtract an integer 2.

```{r}
Zt <- lmod$reTrms$Zt
lambda_mapping_Zt <- sapply(lmod$reTrms$Zt@x, 
                            function(x) which(x == lambda_init)) - 2L
```

We also extract the random effects covariance matrix, with its mapping to $\theta$. See @batesFittingLinearMixedEffects2015 for details.

```{r}
Lambdat <- lmod$reTrms$Lambdat
theta_mapping <- lmod$reTrms$Lind - 1L
```

We can now start by confirming that we are able to compute the likelihood of the model.

```{r}
ml <- marginal_likelihood(
  y = dat$y,
  trials = rep(1, length(dat$y)),
  X = X,
  Zt = Zt,
  Lambdat = Lambdat,
  beta = fixef(irt.model),
  theta = getME(irt.model$lme4, "theta"),
  theta_mapping = theta_mapping,
  lambda = irt.model$Lambda[[1]][, "abil.sid", drop = TRUE][2:3],
  lambda_mapping_X = integer(),
  lambda_mapping_Zt = lambda_mapping_Zt,
  family = "gaussian"
)
```

The `marginal_likelihood` function returns the deviance, which we can compare with that from `PLmixed`:

```{r}
-2 * irt.model$`Log-Likelihood`
ml$deviance
```

The `marginal_likelihood` function also returns the gradient of the marginal likelihood with respect to $\theta$, $\beta$, and $\lambda$, in that order. Reassuringly, it is close to zero. Not however that `PLmixed` returns the maximum of the profile likelihood, which albeit consistent, is not the same as the maximum likelihood estimate [@gongPseudoMaximumLikelihood1981;@parkePseudoMaximumLikelihood1986;@pawitanAllLikelihood2001].

```{r}
ml$gradient
```

This gradient is computed with algorithmic differentiation using the `autodiff` C++ library [@lealAutodiffModernFast2018], which means that it is exact to computer precision.

We can use this gradient to find the maximum likelihood estimates of the model (technically, the Laplace approximate marginal maximum likelihood estimates). We also use `memoise` to avoid evaluating the function unnecessarily.

```{r}
library(memoise)
theta_inds <- 1:2
beta_inds <- 3:5
lambda_inds <- 6:7
bounds <- c(0, 0, rep(-Inf, 5))

mlwrapper <- function(par){
  marginal_likelihood(
    y = dat$y,
    trials = rep(1, length(dat$y)),
    X = X,
    Zt = Zt,
    Lambdat = Lambdat,
    beta = par[beta_inds],
    theta = par[theta_inds],
    theta_mapping = theta_mapping,
    lambda = par[lambda_inds],
    lambda_mapping_X = integer(),
    lambda_mapping_Zt = lambda_mapping_Zt,
    family = "gaussian"
    )
}

mlmem <- memoise(mlwrapper)
fn <- function(par){
  mlmem(par)$deviance
}
gr <- function(par){
  mlmem(par)$gradient
}

```

We start the optimization from some pretty arbitrary values, and it converges quite fast, to a solution which is identical to the one the `PLmixed` found:

```{r}
par_init <- c(1, 1, 0, 0, 0, 1, 1)
opt <- optim(par_init, fn = fn, gr = gr, 
             method = "L-BFGS-B", lower = bounds)

opt$value
-2 * irt.model$`Log-Likelihood`
```

We can confirm that the parameters are identical:

```{r, fig.width=4}
plot(
  opt$par, 
  c(getME(irt.model$lme4, "theta"), fixef(irt.model), 
    irt.model$Lambda[[1]][2:3, 1]),
  xlab = "optim()", ylab = "PLmixed");
abline(0, 1)
```

It should also be clear to the reader the `PLmixed` is MUCH more convenient to use. However, the optimization shown here is faster:

```{r}
system.time({
  opt <- optim(par_init, fn = fn, gr = gr, 
             method = "L-BFGS-B", lower = bounds)
})
```

```{r}
system.time({
  irt.model <- PLmixed(form,
                     data = IRTsub, load.var = c("item"),
                     REML = FALSE, factor = list(c("abil.sid")), 
                     lambda = list(irt.lam), iter.count = FALSE)
})
```


## Generalized linear mixed models with factor structures

For illustrating use with generalized linear mixed models, we start by using a model without any factor structures.

We first fit the model using `lme4`:

```{r}
glmod <- glFormula(cbind(incidence, size - incidence) ~ period + (1 | herd),
                   data = cbpp, family = binomial)
devfun <- do.call(mkGlmerDevfun, glmod)
opt1 <- optimizeGlmer(devfun)
devfun <- updateGlmerDevfun(devfun, glmod$reTrms)
opt2 <- optimizeGlmer(devfun, stage=2)
fMod <- mkMerMod(environment(devfun), opt2, glmod$reTrms, fr = glmod$fr)
```

Then we do it using `marginal_likelihood`:

```{r}
theta_inds <- 1
beta_inds <- 2:5

mlwrapper <- function(par){
  marginal_likelihood(
    y = cbpp$incidence,
    trials = cbpp$size,
    X = glmod$X,
    Zt = glmod$reTrms$Zt,
    Lambdat = glmod$reTrms$Lambdat,
    beta = par[beta_inds],
    theta = par[theta_inds],
    theta_mapping = glmod$reTrms$Lind - 1L,
    lambda = numeric(),
    lambda_mapping_X = integer(),
    lambda_mapping_Zt = integer(),
    family = "binomial"
  )
}

mlmem <- memoise(mlwrapper)
fn <- function(par){
  mlmem(par)$deviance
}
gr <- function(par){
  mlmem(par)$gradient
}

opt <- optim(
  par = c(1, rep(0, 4)), fn = fn, gr = gr,
  method = "L-BFGS-B", lower = c(0, rep(-Inf, 4))
)
```

Using `optim()` with L-BFGS-B yields slightly higher deviance than `lme4`.

```{r}
opt$value
-2 * logLik(fMod)
```

On the other hand, for given parameter values, they perfectly agree, confirming that our implementation is correct, but that some work is needed to check the optimization algorithms.

```{r}
fn(opt2$par)
```




## References
