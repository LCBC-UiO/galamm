---
title: "Maximum Likelihood Estimation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MaximumLikelihood}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ../inst/references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(Matrix)
library(lme4)
library(mgcv)
library(gamm4)
library(galamm)
```


The `galamm` package includes a function for computing the Laplace approximate marginal likelihood of both GLLAMMs and GALAMMs. In the future, this will replace the profile likelihood method, but due to the complexity of the matter, it will take some time to develop a decent API. This vignette describes how to perform maximum likelihood estimation with the currently available functions; it is complicated, but also gives the benefit of understanding what's going on, in addition to a more accurate algorithm which scales better. Before continuing, make sure you've read the Introduction vignette.

## Linear mixed model with factor structure

We will use an example dataset from the `PLmixed` package [@rockwoodEstimatingComplexMeasurement2019]. `PLmixed` is probably a better choice for this model, but this way we get a gentle start before we try estimating generalized multilevel models with semiparametric terms.

```{r}
library(PLmixed)
data("IRTsim")
IRTsub <- IRTsim[IRTsim$item < 4, ]
set.seed(12345)
IRTsub <- IRTsub[sample(nrow(IRTsub), 300), ]
IRTsub <- IRTsub[order(IRTsub$item), ]
irt.lam = c(1, NA, NA)
```

### Estimation with PLmixed

The model can be fit with `PLmixed` as follows, where we're pretending for now the the response is normally distributed, although the binomial distribution would be correct.

```{r, cache=TRUE}
form <- y ~ 0 + as.factor(item) + (0 + abil.sid |sid) +(0 + abil.sid |school)
irt.model <- PLmixed(form,
                     data = IRTsub, load.var = c("item"), # family = binomial,
                     REML = FALSE, factor = list(c("abil.sid")), 
                     lambda = list(irt.lam), iter.count = FALSE)
```

### Estimation with the galamm package

We can use the `marginal_likelihood` function from `galamm` to compute the marginal likelihood of this model at given values of fixed effects $\beta$, variance components $\theta$, and factor loadings $\lambda$.

We start by adding $\lambda$ as a variable in the model:

```{r}
dat <- IRTsub
dat$abil.sid <- 1
```

We then generate the components of a linear mixed model:

```{r}
library(lme4)
lmod <- lFormula(form, data = dat, REML = FALSE)
```

The fixed effects matrix does not contain any factor loadings, so we keep it as is.

```{r}
X <- lmod$X
```

The random effects matrix does contain factor loadings, so we need a mapping. `Zt` is on compressed sparse column format, and `diff(Zt@p)` thus shows us the number of entries in each column of this transposed matrix.

```{r}
Zt <- lmod$reTrms$Zt
table(diff(Zt@p))
lambda_mapping_Zt <- rep(dat$item, each = 2) - 2L
```

We also extract the random effects covariance matrix, with its mapping to $\theta$. See @batesFittingLinearMixedEffects2015 for details.

```{r}
Lambdat <- lmod$reTrms$Lambdat
theta_mapping <- lmod$reTrms$Lind - 1L
```

We can now start by confirming that we are able to compute the likelihood of the model.

```{r}
ml <- marginal_likelihood(
  y = dat$y,
  trials = rep(1, length(dat$y)),
  X = X,
  Zt = Zt,
  Lambdat = Lambdat,
  beta = fixef(irt.model),
  theta = getME(irt.model$lme4, "theta"),
  theta_mapping = theta_mapping,
  lambda = irt.model$Lambda[[1]][, "abil.sid", drop = TRUE][2:3],
  lambda_mapping_X = integer(),
  lambda_mapping_Zt = lambda_mapping_Zt,
  weights = numeric(),
  weights_mapping = integer(),
  family = "gaussian",
  maxit_conditional_modes = 1
)
```

The `marginal_likelihood` function returns the log-likelihood, which we can compare with that from `PLmixed`:

```{r}
irt.model$`Log-Likelihood`
ml$logLik
```

The `marginal_likelihood` function also returns the gradient of the marginal likelihood with respect to $\theta$, $\beta$, and $\lambda$, in that order. Reassuringly, it is close to zero. Not however that `PLmixed` returns the maximum of the profile likelihood, which albeit consistent, is not the same as the maximum likelihood estimate [@gongPseudoMaximumLikelihood1981;@parkePseudoMaximumLikelihood1986;@pawitanAllLikelihood2001].

```{r}
ml$gradient
```

This gradient is computed with algorithmic differentiation using the `autodiff` C++ library [@lealAutodiffModernFast2018], which means that it is exact to computer precision.

We can use this gradient to find the maximum likelihood estimates of the model (technically, the Laplace approximate marginal maximum likelihood estimates). We also use `memoise` to avoid evaluating the function unnecessarily.

```{r}
library(memoise)
theta_inds <- 1:2
beta_inds <- 3:5
lambda_inds <- 6:7
bounds <- c(0, 0, rep(-Inf, 5))

mlwrapper <- function(par){
  marginal_likelihood(
    y = dat$y,
    trials = rep(1, length(dat$y)),
    X = X,
    Zt = Zt,
    Lambdat = Lambdat,
    beta = par[beta_inds],
    theta = par[theta_inds],
    theta_mapping = theta_mapping,
    lambda = par[lambda_inds],
    lambda_mapping_X = integer(),
    lambda_mapping_Zt = lambda_mapping_Zt,
    weights = numeric(),
    weights_mapping = integer(),
    family = "gaussian",
    maxit_conditional_modes = 1
    )
}

mlmem <- memoise(mlwrapper)
fn <- function(par){
  mlmem(par)$logLik
}
gr <- function(par){
  mlmem(par)$gradient
}

```

We start the optimization from some pretty arbitrary values, and it converges quite fast, to a solution which is identical to the one the `PLmixed` found. We set `fnscale = -1` to get the *negative* log-likelihood, which `optim` then minimizes.

```{r}
par_init <- c(1, 1, 0, 0, 0, 1, 1)
opt <- optim(par_init, fn = fn, gr = gr, 
             method = "L-BFGS-B", lower = bounds,
             control = list(fnscale = -1))

opt$value
irt.model$`Log-Likelihood`
```

We can confirm that the parameters are identical:

```{r, fig.width=4}
plot(
  opt$par, 
  c(getME(irt.model$lme4, "theta"), fixef(irt.model), 
    irt.model$Lambda[[1]][2:3, 1]),
  xlab = "optim()", ylab = "PLmixed");
abline(0, 1)
```

It should also be clear to the reader the `PLmixed` is much more convenient to use. However, the optimization shown here is faster. In practice, it is useful to test different values for the `lmm` argument, which defines how many iterations the L-BFGS-B algorithm will keep in memory when approximating the Hessian.

```{r}
set.seed(2123)
system.time({
  opt <- optim(par_init + runif(length(par_init), max = .5), 
               fn = fn, gr = gr, 
             method = "L-BFGS-B", lower = bounds,
             control = list(fnscale = -1))
})
system.time({
  opt <- optim(par_init + runif(length(par_init), max = .5), 
               fn = fn, gr = gr, 
             method = "L-BFGS-B", lower = bounds,
             control = list(fnscale = -1, lmm = 20))
})
```

```{r}
system.time({
  irt.model <- PLmixed(form,
                     data = IRTsub, load.var = c("item"),
                     REML = FALSE, factor = list(c("abil.sid")), 
                     lambda = list(irt.lam), iter.count = FALSE)
})
```

The inverse of the negative Hessian matrix is the covariance matrix of the model. We can obtain this one with a single call at the final parameter estimates, using the argument `hessian = TRUE`. 

```{r}
final_model <- marginal_likelihood(
  y = dat$y,
  trials = rep(1, length(dat$y)),
  X = X,
  Zt = Zt,
  Lambdat = Lambdat,
  beta = opt$par[beta_inds],
  theta = opt$par[theta_inds],
  theta_mapping = theta_mapping,
  lambda = opt$par[lambda_inds],
  lambda_mapping_X = integer(),
  lambda_mapping_Zt = lambda_mapping_Zt,
  weights = numeric(),
  weights_mapping = integer(),
  family = "gaussian",
  maxit_conditional_modes = 1,
  hessian = TRUE
  )
S <- solve(-final_model$hessian)
```

We can confirm that also these standard errors agree with `PLmixed`. Both for $\hat{\beta}$

```{r}
sqrt(diag(S))[beta_inds]
as.data.frame(summary(irt.model)$`Fixed Effects`)[["SE"]]
```

and for $\hat{\lambda}$.

```{r}
sqrt(diag(S))[lambda_inds]
na.omit(as.data.frame(summary(irt.model)$Lambda))[["lambda.item.SE"]]
```


## Generalized linear mixed models

For illustrating use with generalized linear mixed models, we start by using a model without any factor structures.

We first fit the model using `lme4`:

```{r}
glmod <- glFormula(cbind(incidence, size - incidence) ~ period + (1 | herd),
                   data = cbpp, family = binomial)
devfun <- do.call(mkGlmerDevfun, glmod)
opt1 <- optimizeGlmer(devfun)
devfun <- updateGlmerDevfun(devfun, glmod$reTrms)
opt2 <- optimizeGlmer(devfun, stage=2)
fMod <- mkMerMod(environment(devfun), opt2, glmod$reTrms, fr = glmod$fr)
```

Then we do it using `marginal_likelihood`:

```{r}
theta_inds <- 1
beta_inds <- 2:5

mlwrapper <- function(par, hessian = FALSE){
  marginal_likelihood(
    y = cbpp$incidence,
    trials = cbpp$size,
    X = glmod$X,
    Zt = glmod$reTrms$Zt,
    Lambdat = glmod$reTrms$Lambdat,
    beta = par[beta_inds],
    theta = par[theta_inds],
    theta_mapping = glmod$reTrms$Lind - 1L,
    lambda = numeric(),
    lambda_mapping_X = integer(),
    lambda_mapping_Zt = integer(),
    weights = numeric(),
    weights_mapping = integer(),
    family = "binomial",
    maxit_conditional_modes = 50,
    hessian = hessian
  )
}

mlmem <- memoise(mlwrapper)
fn <- function(par){
  mlmem(par)$logLik
}
gr <- function(par){
  mlmem(par)$gradient
}

set.seed(123)
opt <- optim(
  par = c(1, runif(4)), fn = fn, 
  method = "L-BFGS-B", lower = c(0, rep(-Inf, 4)),
  control = list(fnscale = -1, maxit = 2000, lmm = 20)
)
final_model <- mlwrapper(opt$par, hessian = TRUE)
```

Using `optim()` with L-BFGS-B yields slightly higher likelihood than `lme4`.

```{r}
final_model$logLik
logLik(fMod)
```

However, this tiny difference might be due to small numerical differences, as we can see be plugging the `lme4` solution into our function.

```{r}
fn(opt2$par)
```

Also the estimated standard errors are close.

```{r}
sqrt(diag(solve(-final_model$hessian)))[beta_inds]
unname(sqrt(diag(vcov(fMod))))
```



## Generalized additive mixed model with factor structures

### Warm-up



We can now fit the model illustrates in the Introduction vignette, which we refer to for details. We start by adding a weight column to `dat1`, to hold factor loadings.

```{r}
lambda_init <- c(item1 = 1, item2 = 2, item3 = .4)
dat1$weight <- lambda_init[dat1$item]
head(dat1)
```

To confirm that what we do is correct, we fit this GAMM at the initial values of the loadings.

```{r}
mod0 <- gamm4(y ~ 0 + s(x, by = weight), random = ~(0 + weight|id), 
              data = dat1, REML = FALSE)
```


We then compute the mixed model representation of the GAMM.

```{r}
sm <- smoothCon(s(x, by = weight), data = dat1)[[1]]
re <- smooth2random(sm, "", type = 2)
```

Then set up the list which holds the data.

```{r}
mdat <- list(
  id = dat1$id,
  y = dat1$y,
  Xf = re$Xf,
  Xr = re$rand$Xr,
  weight = dat1$weight,
  pseudoGroups = rep(1:ncol(re$rand$Xr), length = nrow(dat1))
)
```

And set up the model.

```{r}
lmod <- lFormula(y ~ 0 + Xf + (1 | pseudoGroups) + (0 + weight | id), 
                 data = mdat, REML = FALSE)
```

Then we add the penalized part of the smooth terms.

```{r}
lmod$reTrms$Ztlist$`1 | pseudoGroups` <- as(t(as.matrix(mdat$Xr))[], class(lmod$reTrms$Zt))
lmod$reTrms$Zt <- rbind(lmod$reTrms$Ztlist$`0 + weight | id`, 
                        lmod$reTrms$Ztlist$`1 | pseudoGroups`)
```

To begin with, we complete model fitting at the initial loading.

```{r}
devfun <- do.call(mkLmerDevfun, lmod)
opt <- optimizeLmer(devfun)
mod1 <- mkMerMod(environment(devfun), opt, lmod$reTrms, fr = lmod$fr)
```

We can now compare the fitted models. The likelihoods are equal.

```{r}
logLik(mod0$mer)
logLik(mod1)
```

As are the fixed effects and variance components.

```{r, fig.width=4}
plot(c(fixef(mod1), getME(mod0$mer, "theta")),
  c(fixef(mod0$mer), getME(mod1, "theta"))); abline(0, 1)
```

We can also compare the penalized spline coefficients.

```{r, fig.width=4}
plot(
  ranef(mod0$mer)$Xr[[1]],
  ranef(mod1)$pseudoGroups[[1]],
  xlab = "gamm4", ylab = "lme4"); abline(0, 1)
```

We now try to convert back to the original spline coefficients.

```{r}
plot(
  re$trans.U %*% (re$trans.D * c(ranef(mod1)$pseudoGroups$`(Intercept)`, fixef(mod1))),
  coef(mod0$gam)
); abline(0, 1)
```

Next, we us `marginal_likelihood` to fit the same model, still treating the factor loadings as fixed.

```{r}
theta_inds <- 1:2
beta_inds <- 3:4
bounds <- c(0, 0, rep(-Inf, 2))

Lambdat <- lmod$reTrms$Lambdat
Lambdat@x <- 1

mlwrapper <- function(par, hessian = FALSE){
  marginal_likelihood(
    y = mod0$gam$y,
    trials = 1,
    X = lmod$X,
    Zt = lmod$reTrms$Zt,
    Lambdat = Lambdat,
    beta = par[beta_inds],
    theta = par[theta_inds],
    theta_mapping = lmod$reTrms$Lind - 1L,
    lambda = numeric(),
    lambda_mapping_X = integer(),
    lambda_mapping_Zt = integer(),
    weights = numeric(),
    weights_mapping = integer(),
    family = "gaussian",
    maxit_conditional_modes = 1, 
    hessian = hessian
    )
}

mlmem <- memoise(mlwrapper)
fn <- function(par){
  mlmem(par)$logLik
}
gr <- function(par){
  mlmem(par)$gradient
}

```

We reach the same maximum likelihood estimate also now.

```{r}
opt <- optim(par = runif(4, 10, 100), fn = fn, gr = gr, 
             method = "L-BFGS-B", lower = bounds, 
             control = list(fnscale = -1, lmm = 20))
opt
```

Also now we can transform the spline coefficients back to their original parametrization.

```{r}
ml_final <- mlwrapper(opt$par, hessian = TRUE)
```

The random effects coming out of `marginal_likelihood` are standard normal, so we need to apply the transformation $\mathbf{b} = \boldsymbol{\Lambda}^{T} \mathbf{u}$ to get random effects on the same scale as `mod0` and `mod1`.

```{r}
Lambdat@x <- opt$par[theta_inds][lmod$reTrms$Lind]
b <- as.numeric(Lambdat[101:108, 101:108] %*% ml_final$u[101:108])
plot(
  c(b, opt$par[beta_inds]),
  c(ranef(mod0$mer)$Xr[[1]], fixef(mod0$mer))
); abline(0, 1)
```

Then we can transform them back to their original parametrization (many steps involved here!!)

```{r}
beta_spline <- re$trans.U %*% (re$trans.D * c(b, opt$par[beta_inds]))
plot(
  beta_spline,
  coef(mod0$gam)
); abline(0, 1)
```

We can also confirm that the smooth functions are the same at the sample values:

```{r}
B <- t(re$trans.D * t(re$trans.U))
Xfp <- cbind(re$rand$Xr, re$Xf)
GXf <- PredictMat(sm, data = dat1)
plot(
  dat1$x, 
  GXf %*% beta_spline,
  col = 2)
points(dat1$x, predict(mod0$gam), col = 3)
```

Finally we find confidence bands for the smooth term. First we need to reproduce the following covariance matrix:

```{r}
vcov(mod0$gam)
```

We now compute it, following the source code of `gamm4::gamm4()`:

```{r}
V <- diag(ml_final$V) + 
  crossprod(Lambdat[1:100, 1:100] %*% lmod$reTrms$Zt[1:100, ]) * ml_final$phi
R <- Matrix::chol(V,pivot=TRUE);piv <- attr(R,"pivot") 
Xfp <- as(Xfp, "dgCMatrix")

WX <- as(solve(t(R), Xfp[piv, ]), "matrix")
Sp <- diag(c(rep(1 / opt$par[[2]], 8), 0, 0))
qrx <- qr(rbind(WX,Sp/sqrt(ml_final$phi)),LAPACK=TRUE)
Ri <- backsolve(qr.R(qrx),diag(ncol(WX)))
ind <- qrx$pivot;ind[ind] <- 1:length(ind)
Ri <- Ri[ind, ]
Vb <- B%*%Ri; Vb <- Vb%*%t(Vb)
```

We can confirm that we landed at the correct covariance matrix:

```{r}
plot(as.numeric(vcov(mod0$gam)), as.numeric(Vb)); abline(0, 1)
```

We then compute confidence bands around the smooth term:

```{r}
Xlp <- PredictMat(sm, data = dat1)
v <- rowSums((Xlp %*% Vb) * Xlp)

inds <- order(dat1$x)[dat1$item == "item1"]
f <- (Xlp %*% beta_spline)
flo <- f - sqrt(v) * 2
fhi <- f + sqrt(v) * 2
plot(
  dat1$x[inds], f[inds], type = "l", ylim = c(0, 13)
)
lines(dat1$x[inds], flo[inds], lty = 2)
lines(dat1$x[inds], fhi[inds], lty = 2)
```



### Maximum likelihood estimation

Having gained some confidence in our ability to fit a GAMM as a mixed model, we can now move on to also estimating the factor loadings.

We first creating mappings between factor loadings and element indices in the design matrices. The lambda mapping for `X` runs through each column.

```{r}
lambda_mapping_X <- rep(as.integer(factor(dat1$item)) - 2L, ncol(lmod$X))
```

The lambda mapping for `Zt` runs through the vector of structural nonzeros. Compressed sparse column format is used, so the `x` vector runs through columns. Each loading is repeated nine times:

```{r}
table(diff(lmod$reTrms$Zt@p))

lambda_mapping_Zt <- rep(-1:1, each = 9, times = 300)

X <- lmod$X / as.numeric(lambda_init[lambda_mapping_X + 2L])
Zt <- lmod$reTrms$Zt
Zt@x <- Zt@x / as.numeric(lambda_init[lambda_mapping_Zt + 2L])
```

We can confirm that they now are free of factor loadings:

```{r}
# Before
head(lmod$X)
# After
head(X)
```

```{r}
compmat <- cbind(before = lmod$reTrms$Zt@x, after = Zt@x)
compmat <- cbind(compmat, ratio = compmat[, 1] / compmat[, 2])

head(compmat, 27)
```

We also remove the estimated values from `Lambdat`.

```{r}
Lambdat <- lmod$reTrms$Lambdat
Lambdat@x <- rep(1, length(Lambdat@x))
```

We then compute the marginal likelihood.

```{r}
margl <- marginal_likelihood(
  y = mdat$y,
  trials = numeric(),
  X = X,
  Zt = Zt,
  Lambdat = Lambdat,
  beta = fixef(mod1),
  theta = getME(mod1, "theta"),
  theta_mapping = lmod$reTrms$Lind - 1L,
  lambda = lambda_init[-1],
  lambda_mapping_X = lambda_mapping_X,
  lambda_mapping_Zt = lambda_mapping_Zt,
  weights = numeric(),
  weights_mapping = integer(),
  family = "gaussian",
  maxit_conditional_modes = 1
)

margl$logLik
logLik(mod1)
```

Finally, we can fit the model directly, using the same memoization technique as above.

```{r}
theta_inds <- 1:2
beta_inds <- 3:4
lambda_inds <- 5:6
bounds <- c(0, 0, rep(-Inf, 4))

mlwrapper <- function(par, hessian = FALSE){
  marginal_likelihood(
    y = mdat$y,
    trials = numeric(),
    X = X,
    Zt = Zt,
    Lambdat = Lambdat,
    beta = par[beta_inds],
    theta = par[theta_inds],
    theta_mapping = lmod$reTrms$Lind - 1L,
    lambda = par[lambda_inds],
    lambda_mapping_X = lambda_mapping_X,
    lambda_mapping_Zt = lambda_mapping_Zt,
    weights = numeric(),
    weights_mapping = integer(),
    family = "gaussian",
    maxit_conditional_modes = 1,
    hessian = hessian
  )
}

mlmem <- memoise(mlwrapper)
fn <- function(par){
  mlmem(par)$logLik
}
gr <- function(par){
  mlmem(par)$gradient
}

```

It ends up at the right values. Profile likelihood estimation fails from these initial values, and is much slower. Note that the deviance is lower now, because the factor loadings are estimated, rather than fixed at $(1, 2, .4)$.

```{r}
par_init <- c(1, 1, 0, 0, 1, 1)

opt <- optim(par_init, fn = fn, gr = gr, 
           method = "L-BFGS-B", lower = bounds, 
           control = list(fnscale = -1, maxit = 2000, lmm = 20))

opt$value
logLik(mod1)
```


```{r}
opt
```

We can also now convert the spline coefficients back to their original parametrization. 

```{r}
ml_final <- mlwrapper(opt$par, hessian = TRUE)

u <- re$trans.U %*% (re$trans.D * c(ml_final$u[101:108], opt$par[beta_inds]))
Lambdat@x <- opt$par[theta_inds][lmod$reTrms$Lind]
b <- c(as.numeric(Lambdat[101:108, 101:108] %*% u[1:8]), u[9:10])

plot(b, coef(mod0$gam)); abline(0, 1)
hist(b - coef(mod0$gam))
```


## References
