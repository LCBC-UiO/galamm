---
title: "Semiparametric Latent Variable Modeling"
output:
  rmarkdown::html_vignette:
    fig_width: 6
    fig_height: 4
bibliography: ../inst/REFERENCES.bib
link-citations: yes
vignette: >
  %\VignetteIndexEntry{Semiparametric Latent Variable Modeling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message=FALSE}
library(galamm)
library(gamm4)
library(ggplot2)
theme_set(theme_bw())
```

This vignette describes how to use `galamm` to estimate latent variable models with smooth terms, or equivalently, generalized additive mixed models with factor structures. The examples are based on Section 4 and 5 in @sorensenLongitudinalModelingAgeDependent2023, but as we cannot share the data, we have instead simulated somewhat simpler datasets that will be used. We will gradually add complexity, starting with a simple generalized additive mixed. Please refer to the [introductory vignette](https://lcbc-uio.github.io/galamm/articles/galamm.html) for an overview of the statistical models.

## Generalized Additive Mixed Models

We start by showing how `galamm` can be used to estimated generalized additive mixed models.

### Gaussian Responses

The `cognition` dataset contains simulated data with measurements of abilities in three cognitive domains.

```{r}
head(cognition)
```

For this first example, we focus only on the first item measured for the first domain.

```{r}
dat <- subset(cognition, domain == 1 & item == "11")
```

Each subject in this dataset has been measured eight times, and we can plot the measurements as follows:

```{r}
ggplot(dat, aes(x = x, y = y, group = id)) +
  geom_point(size = .1) +
  geom_line(alpha = .3)
```

We use a generalized additive mixed model with random intercepts per subject to estimate the function relating $x$ to $y$. In terms of the model framework outlined in the [introductory vignette](https://lcbc-uio.github.io/galamm/articles/galamm.html), we model the $i$th response from the $j$th subject with

$$
y_{ij} = f(x_{ij}) + \eta_{j} + \epsilon_{ij}
$$

where $f(x_{ij})$ is a smooth function to be estimated, $\eta_{j} \sim N(0, \psi)$ is a random intercept, and $\epsilon_{ij} \sim N(0, \phi)$ is a residual term.

This model can be estimated using `gamm4` as follows:

```{r}
mod_gamm4 <- gamm4(y ~ s(x), random = ~ (1 | id), data = dat, REML = FALSE)
```

The package `gamm4` uses `lme4` to fit the underlying model, and the resulting model has two components. `mod_gamm4$mer` contains the mixed model representation, whereas in `mod_gamm4$gam` the fixed and random effects corresponding to spline coefficients have been converted into single smooth terms. We can look at the model summary for each:

```{r}
summary(mod_gamm4$mer)
summary(mod_gamm4$gam)
```

We can also plot the estimated smooth term:

```{r, semiparametric-gaussian-gamm4-smooth}
plot(mod_gamm4$gam)
```

In contrast, invoking the `plot` function on the mixed model part gives us a diagnostic plot.

```{r, semiparametric-gaussian-gamm4-diagnostic}
plot(mod_gamm4$mer)
```


With `galamm` we use similar argument, but the `random` specification is now part of the model formula.

```{r}
mod <- galamm(y ~ s(x) + (1 | id), data = dat)
```

As opposed to `gamm4`, `galamm` gives a single summary. As can be seen, smooth terms are both reported as random effects, and in a separate line under the header "Approximate significance of smooth terms:". Reassuringly, the results from fitting the model with `gamm4` and with `galamm` are essentially equally, even though they use somewhat different computational algorithms.

```{r}
summary(mod)
```

The `plot` function now gives us a diagnostic plot, which by inspection can be seen to be almost identical to the plot produced from the mixed model part of the `gamm4` model.

```{r, semiparametric-gaussian-gamm-diagnostic}
plot(mod)
```

In order to plot the smooth term, we use `plot_smooth`.

```{r, semiparametric-gaussian-gamm-smooth1}
plot_smooth(mod)
```

The `plot_smooth` function is a thin wrapper around the `plot.gam` function provided by the `mgcv` package [@woodGeneralizedAdditiveModels2017a]. This means that the arguments used by `plot.gam` can be used also here, as see with the examples below:

```{r, semiparametric-gaussian-gamm-smooth2}
plot_smooth(mod,
  shade = TRUE, rug = FALSE, seWithMean = TRUE,
  shift = +2
)
plot_smooth(mod, se = FALSE)
```


### Binomial Responses

In the cognition dataset, the responses relating to domain 2 are binomially distributed. We will use the first trial to illustrate how such data can be modeled.

```{r}
dat <- subset(cognition, domain == 2 & item == "21")
```

Again we can fit this model using `gamm4`.

```{r}
mod_gamm4 <- gamm4(y ~ s(x),
  random = ~ (1 | id),
  data = dat, family = binomial
)
```

We can look at the summary output as before.

```{r}
summary(mod_gamm4$mer)
summary(mod_gamm4$gam)
```

And we can plot the smooth term. The diagnostic plot is not very useful in the binomial case, so we omit it.

```{r, semiparametric-gamm4-binomial}
plot(mod_gamm4$gam)
```

Again the `galamm` syntax is similar, but it puts the random effect specification into the model formula.


```{r}
mod <- galamm(y ~ s(x) + (1 | id), data = dat, family = binomial)
```

The estimates are very similar, although not identical. The difference in deviance is due to differences in the way deviance is defined. The call `deviance(mod_gamm4$mer)` gives the same value as in the summary for the model fitted with galamm.

```{r}
summary(mod)
```

```{r, semiparametric-gamm-binomial}
plot_smooth(mod)
```


## Generalized Additive Models with Factor Structures

We now add factor structures to the GAMMs. These are the types of models that neither `gamm4` nor `mgcv` are able to estimate (at least without lots of manual hacking), and where `galamm` provides new functionality.

### Conditionally Gaussian Responses

To illustrate basic usage, we continue with the cognition data, but now use all items of cognitive domain 1. These are all conditionally normal distributed.

```{r}
dat <- subset(cognition, domain == 1)
dat$item <- factor(dat$item)
head(dat)
```

We now need a factor model to associate the underlying latent trait $\eta$ with the measurements $y_{i}$:

$$
y_{i} = \beta_{i} + \lambda_{i} \eta + \epsilon_{i}
$$

In the structural model, we have a smooth term for the relationship between the latent trait and x, and we have random intercepts for a given timepoint within subject $\zeta^{(2)}$, and for a given subject across timepoints $\zeta^{(3)}$.

$$
\eta = h(x) + \zeta^{(2)} + \zeta^{(3)}.
$$

The reduced form of the model is

$$
y_{i} = \beta_{i} + \lambda_{i} \left\{ h(x) + \zeta^{(2)} + \zeta^{(3)} \right\} + \epsilon_{i}
$$

To set up the smooth term, we need a `by`-variable. This is an example of a varying-coefficient term [@hastieVaryingCoefficientModels1993], where $h(x)$ is being interpreted as a regression coefficient for the effect of $\lambda_{i}$ on $y_{i}$, and the regression term varies with $x$. In contrast to @hastieVaryingCoefficientModels1993 and other uses of varying-coefficient terms, however, in this case the predictor $\lambda_{i}$ is a model parameter. We have three items loading in $\eta$ and fix the first loading to 1 for identifiability, so the loading matrix is as follows:

```{r}
(loading_matrix <- matrix(c(1, NA, NA), ncol = 1))
```

We use the term "loading" to identify the factor loading, and set `load.var = "item"` to specify that the loadings to be applied are identified by the "item" variable.

```{r}
mod <- galamm(
  formula = y ~ 0 + item + s(x, by = loading) +
    (0 + loading | id / timepoint),
  data = dat,
  load.var = "item",
  lambda = list(loading_matrix),
  factor = list("loading")
)
```

We print the model summary below. In the data simulation, the factor loadings we set to 1, 1.4, and 0.3, respectively, and this is very well recovered. Furthermore, the ground truth standard deviation at the `id` level was 1, at the `timepoint` level it was 0.5, and the residual standard deviation was 0.1. The estimates are close to these values. Real data will typically not have this strong signal, but based on these results, there are no clear indications that the model is implemented incorrectly.

```{r}
summary(mod)
```

We also plot the smooth term. Since we had a very large amount of data, there is essentially no uncertainty about the estimate.

```{r}
plot_smooth(mod)
```


### Conditionally Binomial Responses

We can now move on to the part of the cognition data that is conditionally binomially distributed. We consider domain 2, where each response measures success or not in a single trial. In this case there are only two items, so we must change the lambda matrix accordingly. Other than that, and setting `family = binomial`, the model is the same as before.

```{r}
dat <- subset(cognition, domain == 2)
dat$item <- factor(dat$item)

mod <- galamm(
  formula = y ~ 0 + item + s(x, by = loading) +
    (0 + loading | id / timepoint),
  data = dat,
  family = binomial,
  load.var = "item",
  lambda = list(matrix(c(1, NA), ncol = 1)),
  factor = list("loading")
)
```

The summary is shown below. The factor loading $\lambda_{2} = 2$ was used when simulating the data, and including the uncertainty, our estimate covers the true value well. Also note that the variation between individuals (group `id`) and the variation between timepoints within individuals (group `timepoint:id`) gets lumped together at the `id` level. The estimated variation at the `timepoint:id` level is zero. This is a well-known phenomenon when fitting mixed models, given book-length treatment in @hodgesRichlyParameterizedLinear2013. In this case, it is likely due to the fact that we only have two measurements at each timepoint, and also the fact that we use the Laplace approximation to integrate over the random effects, and this approximation may be inaccurate for binomial data with a low number of repeated observations [@joeAccuracyLaplaceApproximation2008].


```{r}
summary(mod)
```

The true value 2 for the factor loading is well within the 95 % confidence limits.

```{r}
confint(mod, parm = "lambda")
```


While `gamm4` is not able to estimate the factor loadings, it is able to fit the model if we treat the estimated factor loadings as data. We confirm this below, where we add the estimated loadings as data. Some changes also have to be done to the formula, due to how smooths are set up by `mgcv` and `gamm4`. Explaining this is beyond the scope of this vignette, but set `?mgcv::gam.models` for details.

```{r}
dat_gamm4 <- dat
dat_gamm4$loading <- ifelse(
  dat_gamm4$item == "21", 1,
  factor_loadings(mod)["lambda2", "loading"]
)
dat_gamm4$item2 <- as.integer(dat$item == "22")
head(dat_gamm4)

mod_gamm4 <- gamm4(
  formula = y ~ 0 + item2 + s(x, by = loading),
  random = ~ (0 + loading | id / timepoint),
  data = dat_gamm4,
  family = binomial
)
```

We see that the log-likelihood are very similar, but that the GALAMM uses one more degree of freedom than the GAMM, due to the factor loading that is being estimated.

```{r}
# galamm model with estimated factor loadings
logLik(mod)
# gamm4 model with fixed factor loadings
logLik(mod_gamm4$mer)
```

We can also compare the smooth terms visually. In order to do this, we must impose sum-to-zero constraints on the GAMM fit. We also include the data-generating function with a blue curve. For the GALAMM, this has been done automatically. Although not identical, it is clear that the two terms are very close.

```{r}
# gamm4 fit
nd <- dat_gamm4
nd$loading <- 1
nd <- nd[order(nd$x), ]
gamm4_fit <- predict(mod_gamm4$gam, newdata = nd)
gamm4_fit <- gamm4_fit - mean(gamm4_fit)

# true function
x <- seq(0, 1, length.out = 100)
true_fun <- function(x) {
  exp(2 * x)
}
y <- true_fun(x)
y <- y - mean(y)

# plot them
plot_smooth(mod, main = "Comparison of smooth terms", shade = TRUE)
lines(nd$x, gamm4_fit, col = "red")
lines(x, y, col = "blue")
legend(
  x = 0, y = 4, legend = c("GALAMM", "GAMM4", "True"),
  col = c("black", "red", "blue"), lty = 1
)
```


### Multivariate Gaussian Model

We now do a joint analysis of domain 1 and domain 3, for which all item responses are conditionally normally distributed.

Letting $\eta_{1}$ denote latent ability in domain 1 and $\eta_{3}$ denote latent ability in domain 3, and $\lambda_{i1}$ and $\lambda_{i3}$ be corresponding factor loadings for the $i$th item measuring each domain, the measurement model is now

$$
y_{i} = \beta_{ij} + \lambda_{ij} \eta_{j} + \epsilon_{ij} ~ j=1,3
$$

To avoid unnecessary complexity, we assume the residual standard deviation is the same for all responses. The [vignette on linear mixed models with heteroscedastic residuals](https://lcbc-uio.github.io/galamm/articles/lmm_heteroscedastic.html) shows how this assumption can be relaxed. The data were also simulated with all residual standard deviations equal, so in this case the homoscedasticity assumption is satisfied.

In the structural model, we have a smooth term for the relationship between the latent trait and x, and we have random intercepts for a given timepoint within subject $\zeta^{(2)}$, and for a given subject across timepoints $\zeta^{(3)}$.

$$
\eta_{j} = h_{j}(x) + \zeta_{j}^{(2)} + \zeta_{j}^{(3)} ~j=1,3.
$$

We first subset the cognition dataset to get the observations we want.

```{r}
dat <- subset(cognition, domain %in% c(1, 3))
dat$domain <- factor(dat$domain)
dat$item <- factor(dat$item)
```


It also makes life easier when setting up the model below, if we have have dummy variables for the items ready in our dataframe. We create this below. We drop the first items in each domain, since these will be captured by the overall intercept of the smooth terms.

```{r}
mm <- model.matrix(~ 0 + item, data = dat)
dat <- cbind(dat, mm[, c("item12", "item13", "item32", "item33", "item34")])
```

To build up the complexity step-by-step, and also make sure that the model is correctly set up, we start by fixing the factor loadings to their true values.


```{r}
dat$loading1 <- c(1, 1.4, 3, 0, 0, 0, 0)[dat$item]
dat$loading3 <- c(0, 0, 0, 1, 1, 1, 2)[dat$item]
```

We then fit the same model using `gamm4` and `galamm`. Some comments to the formula are useful here.

First, the varying-coefficient terms `s(x, by = loading1, k = 10)` and `s(x, by = loading3, k = 10)` each contain an intercept, and since `loading1` is zero for all items in domain 3 and `loading3` is zero for all terms in domain 1, these functions contribute one intercept to each domain. Thus, we need to remove the overall intercept by starting the formula with `0 + `. We use the first item in each domain as a reference level, so the effects of `item12` and `item13` are the difference between these items and `item11`, with the latter intercept being captured by the intercept of `s(x, by = loading1, k = 10)`. The same applies to the items in domain 3. 

Considering the random effect specification, `(0 + loading1 + loading3 | id)` specifies that there are two latent variables at the subject level ($\eta_{1}$ and $\eta_{3}$), their factor loadings are given by `loading1` and `loading3`, and they are allowed to be correlated. Next, `(0 + loading1 | id : timepoint) + (0 + loading3 | id : timepoint)` specifies that there are latent variables also varying between timepoints of the same individual, but by splitting the terms, we specify that these latent variables are not correlated. We do this for two reasons: first, including correlations between latent variables at multiple levels typically leads to identifiability issues. Secondly, when simulating the data, the level-2 latent variables were set to be uncorrelated, and hence in this case the assumption is satisfied.

```{r}
mod_gamm4 <- gamm4(
  formula = y ~ 0 + item12 + item13 + item32 + item33 + item34 +
    s(x, by = loading1, k = 10) +
    s(x, by = loading3, k = 10),
  random = ~ (0 + loading1 + loading3 | id) +
    (0 + loading1 | id:timepoint) +
    (0 + loading3 | id:timepoint),
  data = dat,
  REML = FALSE
)

mod <- galamm(
  formula = y ~ 0 + item12 + item13 + item32 + item33 + item34 +
    s(x, by = loading1, k = 10) +
    s(x, by = loading3, k = 10) +
    (0 + loading1 + loading3 | id) +
    (0 + loading1 | id:timepoint) +
    (0 + loading3 | id:timepoint),
  data = dat
)
```

We confirm that the model summary outputs are practically identical.

```{r}
summary(mod_gamm4$mer)
summary(mod_gamm4$gam)
summary(mod)
```


We also plot the two smooth terms, and confirm that they are indentical.

```{r}
plot_smooth(mod, pages = 1, scale = 0, shade = TRUE)
plot(mod_gamm4$gam, pages = 1, scale = 0, shade = TRUE)
```

```{r}
dat$loading1 <- dat$loading3 <- NULL
```

We define the loading matrix, with one column for each domain.

```{r}
(lambda <- matrix(c(
  1, NA, .3, 0, 0, 0, 0,
  0, 0, 0, 1, 1, 1, NA
), ncol = 2))
```

At this point we enter a part of the package which is under active development.

```{r, eval=FALSE}
mod <- galamm(
  formula = y ~ 0 + item12 + item13 + item32 + item33 + item34 +
    s(x, by = loading1, k = 4) +
    s(x, by = loading3, k = 4) +
    (0 + loading1 + loading3 | id) +
    (0 + loading1 | id:timepoint) +
    (0 + loading3 | id:timepoint),
  data = dat,
  load.var = "item",
  lambda = list(lambda),
  factor = list(c("loading1", "loading3")),
  control = galamm_control(optim_control = list(maxit = 3, trace = 3))
)
```


# References
