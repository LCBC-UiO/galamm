---
title: "Models with Mixed Response Types"
output:
  rmarkdown::html_vignette:
    fig_width: 6
    fig_height: 4
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteIndexEntry{Models with Mixed Response Types}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(galamm)
library(lme4)
```

This vignette describes how `galamm` can be used to estimate models with mixed response types.

## Mixed Normal and Binomial Response

We start with the `mresp` dataset, which comes with the package. The variable "itemgroup" defines the response type; it equals "a" for normally distributed responses and "b" for binomially distributed responses. They are link through a common random intercept.

```{r}
head(mresp)
```


In terms of the GALAMM defined in the [introductory vignette](https://lcbc-uio.github.io/galamm/articles/introduction.html), and for simplicity assuming we use canonincal link functions, we have the response model

$$
f\left(y_{ij} | \nu_{ij}, \phi\right) = \exp \left( \frac{y_{ij}\nu_{ij} - b\left(\nu_{ij}\right)}{\phi} + c\left(y_{ij}, \phi\right) \right)
$$

for the $i$th observation of the $j$ subject. Although we don't show this with a subscript, when the variable `itemgroup = "a"` we have a Gaussian response, so $b(\nu) = \nu^{2}/2$ and the support of the distribution is the entire real line $\mathbb{R}$. The mean in this case is given by $\mu_{ij} = \nu_{ij}$. When `itemgroup = "b"` we have a binomial response, so $b(\nu) = \log(1 + \exp(\nu))$ and the support is $\{0, 1\}$. In this binomial case we also have $\phi=1$. The mean in this case is given by $\mu_{ij} = \exp(\nu_{ij}) / (1 + \exp(\nu_{ij}))$. The function $c(y_{ij}, \phi)$ also differs between these cases, but is not of the same interest, since it does not depend on the linear predictor. It hence matters for the value of the log-likelihood, but not for its derivative with respect to the parameters of interest.

Next, the nonlinear predictor is given by

$$\nu_{ij} = \beta_{0} + x_{ij}\beta_{1} + \mathbf{z}_{ij}^{T}\boldsymbol{\lambda} \eta$$

where $x_{ij}$ is an explanatory and $\mathbf{z}_{ij}$ is a dummy vector of length 2 with exactly one element equal to one and one element equal to zero. When `itemgroup = "a"`, $\mathbf{z} = (1, 0)^{T}$ and when `itemgroup = "b"`, $\mathbf{z} = (0, 1)^{T}$. The parameter $\boldsymbol{\lambda} = (1, \lambda)^{T}$ is a vector of factor loadings, whose first element equals zero for identifiability. $\eta$ is a latent variable, in this case representing an underlying trait "causing" the observed responses.

The structural model is simply

$$
\eta = \zeta \sim N(0, \psi),
$$

where $N(0, \psi)$ denotes a normal distribution with mean 0 and variance $\psi$.

We define the loading matrix as follows, where the value `1` indicates that the first element $\boldsymbol{\lambda}$ is fixed, and the value `NA` indicates that its second element is unknown, and to be estimated.

```{r}
(loading_matrix <- matrix(c(1, NA), ncol = 1))
```

We set `load.var = "itemgroup"` because all rows of the data with the same value of `itemgroup` will receive the same factor loading. In `formula`, we state `(0 + level | id)` to specify that each subject, identified by `id`, has a given level. `level` is not an element of the `mresp` data, but is instead the factor onto which the loading matrix loads. We identify this with the argument `factor = list("level")`. We could have chosen any other name for `"level"`, except for names that are already columns in `mresp`. 

The argument `family = c(gaussian, binomial)` specifies that we have both Gaussian and binomially distributed responses. `family_mapping = ifelse(mresp$itemgroup == "a", 1L, 2L)` says that is `itemgroup` is `"a"` then take the first element of `family`, otherwise take the second element. In this way we specify which row of the response vector is distributed according to which distribution.

```{r}
mixed_resp <- galamm(
  formula = y ~ x + (0 + level | id),
  data = mresp,
  family = c(gaussian, binomial),
  family_mapping = ifelse(mresp$itemgroup == "a", 1L, 2L),
  load.var = "itemgroup",
  lambda = list(loading_matrix),
  factor = list("level")
)

summary(mixed_resp)
```


## Covariate Measurement Error Model

This example is taken from Chapter 14.2 in @skrondalGeneralizedLatentVariable2004, which I refer to for further interpretation and discussion. The main purpose here is to confirm that we reproduce their results, and to show the syntax.

The model is defined as follows. First, true fiber intake for subject $j$ is assumed to depend on age and on whether the person is a bus driver or banking staff, and their interaction.

$$\eta_{j} = \mathbf{x}_{j}'\boldsymbol{\gamma} + \zeta_{j}.$$

For a part of the sample, there are two measurements of fiber intake, and we can hence estimate a measurement model, as follows:

$$y_{ij} = \eta_{j} + \epsilon_{ij}, \qquad \epsilon_{ij} = N(0, \theta)$$

Finally we define a disease model for the probability of coronary heart disease, as

$$\text{logit}[P(D_{j}=1 | \eta_{j})] = \mathbf{x}_{j}'\boldsymbol{\beta} + \lambda \eta_{j}.$$

Stacking the three responses, which are fiber intake at times 1 and 2, and coronary heart disease, we can define the joint model as a GLLAMM with linear predictor

$$\nu_{ij} = d_{3i} \mathbf{x}_{j}'\boldsymbol{\beta} + \mathbf{x}_{j}'\boldsymbol{\gamma}\left[(d_{1i} + d_{2i}) + \lambda d_{3i}\right] + \zeta_{j} \left[(d_{1i} + d_{2i}) + \lambda d_{3i}\right],$$

where $d_{1i}$ and $d_{2i}$ are indicators for fiber measurements at timepoints 1 and 2, and $d_{3i}$ is an indicator for coronary heart disease.

At this point it may help to look at the first few rows of the dataset.

```{r}
head(diet, 10)
```

With this dataset, the response $y$ is normally distributed when $d_{1i}=1$ or $d_{2i}=1$, but binomially distributed when $d_{3i}=1$. These two response processes are connected through the latent variable $\eta_{j}$.

We compute estimates, but note that the Hessian is rank deficient.

```{r}
lam <- matrix(c(1, 1, NA), ncol = 1)
mod <- galamm(
  formula = y ~ item + (age * bus):chd
    + (age * bus):loading:fiber + (0 + loading | id),
  data = diet,
  family = c(gaussian, binomial),
  family_mapping = ifelse(diet$item == "chd", 2L, 1L),
  factor = list("loading"),
  load.var = "item",
  lambda = list(lam)
)
```

Inspecting the parameters shows that the latent variable is estimated to have zero variance. We try to increase the initial value, and see what happens.


```{r}
mod <- galamm(
  formula = y ~ item + (age * bus):chd + (age * bus):loading:fiber + (0 + loading | id),
  data = diet,
  family = c(gaussian, binomial),
  family_mapping = ifelse(diet$item == "chd", 2L, 1L),
  factor = list("loading"),
  load.var = "item",
  lambda = list(lam),
  start = list(theta = 10)
)
```

```{r}
summary(mod)
```


Now we were able to reproduce results very close to those in Table 14.1 on page 420 of @skrondalGeneralizedLatentVariable2004.

# References
